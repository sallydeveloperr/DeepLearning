{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb8a872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from requests->kagglehub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from requests->kagglehub) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dfc04a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sally\\.cache\\kagglehub\\datasets\\aashita\\nyt-comments\\versions\\13\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6822b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "article_lists = glob(path+'/*.*',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55766074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def9f57c459f24986d7c90</td>\n",
       "      <td>575</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
       "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-04-01 00:53:06</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A court ruling annulling the legislature’s aut...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58defd317c459f24986d7c95</td>\n",
       "      <td>1374</td>\n",
       "      <td>By MICHAEL POWELL</td>\n",
       "      <td>article</td>\n",
       "      <td>Stain Permeates Basketball Blue Blood</td>\n",
       "      <td>['Basketball (College)', 'University of North ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-01 01:06:52</td>\n",
       "      <td>College Basketball</td>\n",
       "      <td>For two decades, until 2013, North Carolina en...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58df09b77c459f24986d7ca7</td>\n",
       "      <td>708</td>\n",
       "      <td>By DEB AMLEN</td>\n",
       "      <td>article</td>\n",
       "      <td>Taking Things for Granted</td>\n",
       "      <td>['Crossword Puzzles']</td>\n",
       "      <td>3</td>\n",
       "      <td>Games</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01 02:00:14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract                 articleID  articleWordCount  \\\n",
       "0      NaN  58def1347c459f24986d7c80               716   \n",
       "1      NaN  58def3237c459f24986d7c84               823   \n",
       "2      NaN  58def9f57c459f24986d7c90               575   \n",
       "3      NaN  58defd317c459f24986d7c95              1374   \n",
       "4      NaN  58df09b77c459f24986d7ca7               708   \n",
       "\n",
       "                                byline documentType  \\\n",
       "0  By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                      By GAIL COLLINS      article   \n",
       "2               By THE EDITORIAL BOARD      article   \n",
       "3                    By MICHAEL POWELL      article   \n",
       "4                         By DEB AMLEN      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                  And Now,  the Dreaded Trump Curse   \n",
       "2              Venezuela’s Descent Into Dictatorship   \n",
       "3              Stain Permeates Basketball Blue Blood   \n",
       "4                          Taking Things for Granted   \n",
       "\n",
       "                                            keywords  multimedia    newDesk  \\\n",
       "0  ['Photography', 'New York Times', 'Niger', 'Fe...           3    Insider   \n",
       "1  ['United States Politics and Government', 'Tru...           3       OpEd   \n",
       "2  ['Venezuela', 'Politics and Government', 'Madu...           3  Editorial   \n",
       "3  ['Basketball (College)', 'University of North ...           3     Sports   \n",
       "4                              ['Crossword Puzzles']           3      Games   \n",
       "\n",
       "   printPage              pubDate         sectionName  \\\n",
       "0          2  2017-04-01 00:15:41             Unknown   \n",
       "1         23  2017-04-01 00:23:58             Unknown   \n",
       "2         22  2017-04-01 00:53:06             Unknown   \n",
       "3          1  2017-04-01 01:06:52  College Basketball   \n",
       "4          0  2017-04-01 02:00:14             Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  One of the largest photo displays in Times his...  The New York Times   \n",
       "1                  Meet the gang from under the bus.  The New York Times   \n",
       "2  A court ruling annulling the legislature’s aut...  The New York Times   \n",
       "3  For two decades, until 2013, North Carolina en...  The New York Times   \n",
       "4  In which Howard Barkin and Will Shortz teach u...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1          Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  \n",
       "2      Editorial  https://www.nytimes.com/2017/03/31/opinion/ven...  \n",
       "3           News  https://www.nytimes.com/2017/03/31/sports/ncaa...  \n",
       "4           News  https://www.nytimes.com/2017/03/31/crosswords/...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(article_lists[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f597e41",
   "metadata": {},
   "source": [
    "##### LSTM \n",
    "입력 : \"나는 파이썬을 좋아합니다. 따라서 나는 ___을 잘합니다.\"\n",
    "- 일반신경망 : 공부(파이썬 정보가 희석됨)\n",
    "- LSTM : 프로그래밍(오래된 정보도 기억함)\n",
    "\n",
    "- LSTM의 핵심 키워드\n",
    "    - 장기기억 : 중요한 정보는 오래 기억\n",
    "    - 단기기억 : 불필요한 정보는 버림\n",
    "    - 순서이해 : 시간 순서를 이해함\n",
    "\n",
    "##### 3개의 Gate를 통해 정보의 흐름을 제어\n",
    "- LSTM의 셀은 한 시점에 t\n",
    "    - 입력 : $x_t$(현재데이터)\n",
    "    - 이전 은닉상태 : $h_{t-1}$\n",
    "    - 이전 셀 상태 : $c_{t-1}$\n",
    "\n",
    "    --> forget gate --> input gate  --> output gate\n",
    "        잊을 데이터     추가할 데이터    출력할 데이터\n",
    "\n",
    "    출력 : $h_t$, $c_t$\n",
    "    - 현재 데이터 $h_t$가 $c_t$로 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb25ef",
   "metadata": {},
   "source": [
    "Forget Gate(잊음 관문)\n",
    "\n",
    "$f_t = s(w_f.[h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "s: sigmoid 함수 (0~1)\n",
    "\n",
    "이전 셀 상태 : [1.5, -0.3, 2.1]\n",
    "\n",
    "현제 입력 : '새로운 문장 입력'\n",
    "\n",
    "$f_t$ = [0.1,0.05,0.9]\n",
    "\n",
    "결과 : [1.5 * 0.1, -0.3 * 0.05, 2.1 * 0.9] = [0.15, -0.015, 1.89]\n",
    "\n",
    "첫 2개는 버리고 3번째는 유지한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da037e3c",
   "metadata": {},
   "source": [
    "##### input gate : 입력에 관련된 문\n",
    "- 결과가 나오면 70받고, 두번째는 30% 받음\n",
    "\n",
    "##### Cell State 업데이트\n",
    " - 이전 기억에서 필요한 것만 유지하고 새로운 정보에서 필요한 것만 유지\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d943a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headline = []\n",
    "articles = [path for path in article_lists if \"Articles\" in path]\n",
    "# headline 정보만 추출 all_headline에 추가\n",
    "# 전처리 : 소문자로 변경하고 특수문자 제거\n",
    "import string\n",
    "\n",
    "for a in articles:\n",
    "    df = pd.read_csv(a)                     # 파일 읽기\n",
    "    if \"headline\" in df.columns:            # 컬럼 존재 확인 (안전)\n",
    "        all_headline.extend(df[\"headline\"].values)  # ✅ df에서 headline 추출\n",
    "    else:\n",
    "        print(f\"[경고] {a} 파일에 'headline' 컬럼이 없습니다.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71916e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finding': 0,\n",
       " 'an': 1,\n",
       " 'expansive': 2,\n",
       " 'view': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'forgotten': 6,\n",
       " 'people': 7,\n",
       " 'in': 8,\n",
       " 'niger': 9,\n",
       " 'and': 10,\n",
       " 'now,': 11,\n",
       " 'the': 12,\n",
       " 'dreaded': 13,\n",
       " 'trump': 14,\n",
       " 'curse': 15,\n",
       " 'venezuela’s': 16,\n",
       " 'descent': 17,\n",
       " 'into': 18,\n",
       " 'dictatorship': 19,\n",
       " 'stain': 20,\n",
       " 'permeates': 21,\n",
       " 'basketball': 22,\n",
       " 'blue': 23,\n",
       " 'blood': 24,\n",
       " 'taking': 25,\n",
       " 'things': 26,\n",
       " 'for': 27,\n",
       " 'granted': 28,\n",
       " 'caged': 29,\n",
       " 'beast': 30,\n",
       " 'awakens': 31,\n",
       " 'ever-unfolding': 32,\n",
       " 'story': 33,\n",
       " 'o’reilly': 34,\n",
       " 'thrives': 35,\n",
       " 'as': 36,\n",
       " 'settlements': 37,\n",
       " 'add': 38,\n",
       " 'up': 39,\n",
       " 'mouse': 40,\n",
       " 'infestation': 41,\n",
       " 'divide': 42,\n",
       " 'g.o.p.': 43,\n",
       " 'now': 44,\n",
       " 'threatens': 45,\n",
       " 'tax': 46,\n",
       " 'plan': 47,\n",
       " 'variety': 48,\n",
       " 'puzzle:': 49,\n",
       " 'acrostic': 50,\n",
       " 'they': 51,\n",
       " 'can': 52,\n",
       " 'hit': 53,\n",
       " 'ball': 54,\n",
       " '400': 55,\n",
       " 'feet.': 56,\n",
       " 'but': 57,\n",
       " 'play': 58,\n",
       " 'catch?': 59,\n",
       " 'that’s': 60,\n",
       " 'tricky.': 61,\n",
       " 'country,': 62,\n",
       " 'shock': 63,\n",
       " 'at': 64,\n",
       " 'budget': 65,\n",
       " 'cuts': 66,\n",
       " 'why': 67,\n",
       " 'is': 68,\n",
       " 'this': 69,\n",
       " 'hate': 70,\n",
       " 'different': 71,\n",
       " 'from': 72,\n",
       " 'all': 73,\n",
       " 'other': 74,\n",
       " 'hate?': 75,\n",
       " 'pick': 76,\n",
       " 'your': 77,\n",
       " 'favorite': 78,\n",
       " 'ethical': 79,\n",
       " 'offender': 80,\n",
       " 'my': 81,\n",
       " 'son’s': 82,\n",
       " 'growing': 83,\n",
       " 'black': 84,\n",
       " 'pride': 85,\n",
       " 'jerks': 86,\n",
       " 'start-ups': 87,\n",
       " 'ruin': 88,\n",
       " 'needs': 89,\n",
       " 'brain': 90,\n",
       " 'manhood': 91,\n",
       " 'age': 92,\n",
       " 'value': 93,\n",
       " 'college': 94,\n",
       " 'initial': 95,\n",
       " 'description': 96,\n",
       " 'rough': 97,\n",
       " 'estimates': 98,\n",
       " 'el': 99,\n",
       " 'pasatiempo': 100,\n",
       " 'nacional': 101,\n",
       " 'cooling': 102,\n",
       " 'off': 103,\n",
       " 'on': 104,\n",
       " 'hot': 105,\n",
       " 'day': 106,\n",
       " 'yankee': 107,\n",
       " 'stadium': 108,\n",
       " 'trump’s': 109,\n",
       " 'staff': 110,\n",
       " 'mixed': 111,\n",
       " 'politics': 112,\n",
       " 'paydays': 113,\n",
       " 'virtuoso': 114,\n",
       " 'rebuilding': 115,\n",
       " 'act': 116,\n",
       " 'requires': 117,\n",
       " 'everyone': 118,\n",
       " 'tune': 119,\n",
       " '‘homeland,’': 120,\n",
       " 'season': 121,\n",
       " '6,': 122,\n",
       " 'episode': 123,\n",
       " '11:': 124,\n",
       " 'quinn': 125,\n",
       " 'just': 126,\n",
       " 'natural': 127,\n",
       " 'killer?': 128,\n",
       " '‘big': 129,\n",
       " 'little': 130,\n",
       " 'lies’': 131,\n",
       " 'art': 132,\n",
       " 'empathy': 133,\n",
       " 'upending': 134,\n",
       " 'whodunit': 135,\n",
       " '‘feud:': 136,\n",
       " 'bette': 137,\n",
       " 'joan’': 138,\n",
       " '5:': 139,\n",
       " 'stage': 140,\n",
       " '‘billions’': 141,\n",
       " '2,': 142,\n",
       " '7:': 143,\n",
       " 'greed': 144,\n",
       " 'good.': 145,\n",
       " 'except': 146,\n",
       " 'when': 147,\n",
       " 'it’s': 148,\n",
       " 'not.': 149,\n",
       " 'unknown': 150,\n",
       " 'what’s': 151,\n",
       " 'going': 152,\n",
       " 'picture?': 153,\n",
       " '|': 154,\n",
       " 'april': 155,\n",
       " '3,': 156,\n",
       " '2017': 157,\n",
       " 'have': 158,\n",
       " 'you': 159,\n",
       " 'ever': 160,\n",
       " 'felt': 161,\n",
       " 'pressured': 162,\n",
       " 'by': 163,\n",
       " 'family': 164,\n",
       " 'or': 165,\n",
       " 'others': 166,\n",
       " 'making': 167,\n",
       " 'important': 168,\n",
       " 'decision': 169,\n",
       " 'about': 170,\n",
       " 'future?': 171,\n",
       " 'cornerstone': 172,\n",
       " 'peace': 173,\n",
       " 'risk': 174,\n",
       " 'wimping': 175,\n",
       " 'out': 176,\n",
       " 'trade': 177,\n",
       " 'dwindling': 178,\n",
       " 'odds': 179,\n",
       " 'coincidence': 180,\n",
       " 'what': 181,\n",
       " 'was': 182,\n",
       " 'lenin': 183,\n",
       " 'thinking?': 184,\n",
       " 'mitch': 185,\n",
       " 'mcconnell’s': 186,\n",
       " 'trigger': 187,\n",
       " 'finger': 188,\n",
       " 'ad': 189,\n",
       " 'north': 190,\n",
       " 'korea': 191,\n",
       " 'yields': 192,\n",
       " 'nuclear': 193,\n",
       " 'clues': 194,\n",
       " 'good': 195,\n",
       " 'news': 196,\n",
       " 'older': 197,\n",
       " 'mothers': 198,\n",
       " 'does': 199,\n",
       " 'birth': 200,\n",
       " 'control': 201,\n",
       " 'cause': 202,\n",
       " 'depression?': 203,\n",
       " 'turning': 204,\n",
       " 'negative': 205,\n",
       " 'thinkers': 206,\n",
       " 'positive': 207,\n",
       " 'ones': 208,\n",
       " 'new': 209,\n",
       " 'york': 210,\n",
       " 'today:': 211,\n",
       " 'belated': 212,\n",
       " 'middlebury,': 213,\n",
       " 'divided': 214,\n",
       " 'campus': 215,\n",
       " 'democrats’': 216,\n",
       " 'vow': 217,\n",
       " 'to': 218,\n",
       " 'bar': 219,\n",
       " 'gorsuch': 220,\n",
       " 'sets': 221,\n",
       " 'clash': 222,\n",
       " 'h-1b': 223,\n",
       " 'visa': 224,\n",
       " 'applications': 225,\n",
       " 'pour': 226,\n",
       " 'truckload': 227,\n",
       " 'amplified': 228,\n",
       " 'world,': 229,\n",
       " 'critics': 230,\n",
       " 'take': 231,\n",
       " 'aim': 232,\n",
       " 'referees': 233,\n",
       " 'revered': 234,\n",
       " 'milwaukee': 235,\n",
       " 'restaurant,': 236,\n",
       " 'karl': 237,\n",
       " 'ratzsch,': 238,\n",
       " 'says': 239,\n",
       " 'goodbye': 240,\n",
       " 'n.h.l.': 241,\n",
       " 'its': 242,\n",
       " 'players': 243,\n",
       " 'will': 244,\n",
       " 'skip': 245,\n",
       " '2018': 246,\n",
       " 'olympics': 247,\n",
       " 'company': 248,\n",
       " 'classic': 249,\n",
       " 'nasty': 250,\n",
       " 'woman': 251,\n",
       " '22': 252,\n",
       " 'ways': 253,\n",
       " 'teach': 254,\n",
       " 'learn': 255,\n",
       " 'poetry': 256,\n",
       " 'with': 257,\n",
       " 'times': 258,\n",
       " 'search': 259,\n",
       " 'king': 260,\n",
       " 'solomon’s': 261,\n",
       " 'pantry': 262,\n",
       " 'back': 263,\n",
       " 'move': 264,\n",
       " 'forward': 265,\n",
       " 'peek': 266,\n",
       " 'white': 267,\n",
       " 'house': 268,\n",
       " 'swamp': 269,\n",
       " 'disney': 270,\n",
       " 'character': 271,\n",
       " 'living': 272,\n",
       " 'quirky': 273,\n",
       " 'dream': 274,\n",
       " 'justice': 275,\n",
       " 'dept.': 276,\n",
       " 're-examine': 277,\n",
       " 'police': 278,\n",
       " 'accords': 279,\n",
       " 'year,': 280,\n",
       " 'their': 281,\n",
       " 'year': 282,\n",
       " 'latest': 283,\n",
       " 'health': 284,\n",
       " 'proposal': 285,\n",
       " 'weakens': 286,\n",
       " 'coverage': 287,\n",
       " 'pre-existing': 288,\n",
       " 'conditions': 289,\n",
       " 'losing': 290,\n",
       " 'let’s': 291,\n",
       " 'go': 292,\n",
       " 'win': 293,\n",
       " 'opioids': 294,\n",
       " 'florida’s': 295,\n",
       " 'vengeful': 296,\n",
       " 'governor': 297,\n",
       " 'how': 298,\n",
       " 'end': 299,\n",
       " 'politicization': 300,\n",
       " 'courts': 301,\n",
       " 'dr.': 302,\n",
       " 'came': 303,\n",
       " 'against': 304,\n",
       " 'vietnam': 305,\n",
       " 'britain’s': 306,\n",
       " 'trains': 307,\n",
       " 'don’t': 308,\n",
       " 'run': 309,\n",
       " 'time.': 310,\n",
       " 'blame': 311,\n",
       " 'capitalism.': 312,\n",
       " 'questions': 313,\n",
       " 'for:': 314,\n",
       " '‘no': 315,\n",
       " 'license': 316,\n",
       " 'plates': 317,\n",
       " 'here:': 318,\n",
       " 'using': 319,\n",
       " 'transcend': 320,\n",
       " 'prison': 321,\n",
       " 'walls’': 322,\n",
       " 'dry': 323,\n",
       " 'spell': 324,\n",
       " 'are': 325,\n",
       " 'there': 326,\n",
       " 'subjects': 327,\n",
       " 'that': 328,\n",
       " 'should': 329,\n",
       " 'be': 330,\n",
       " 'off-limits': 331,\n",
       " 'artists,': 332,\n",
       " 'certain': 333,\n",
       " 'artists': 334,\n",
       " 'particular?': 335,\n",
       " '‘that': 336,\n",
       " 'great': 337,\n",
       " 'television’': 338,\n",
       " 'thinking': 339,\n",
       " 'code': 340,\n",
       " 'gorsuch’s': 341,\n",
       " 'influence': 342,\n",
       " 'could': 343,\n",
       " 'greater': 344,\n",
       " 'than': 345,\n",
       " 'his': 346,\n",
       " 'vote': 347,\n",
       " 'ease': 348,\n",
       " 'hangover': 349,\n",
       " 'gifts': 350,\n",
       " 'china': 351,\n",
       " 'penn': 352,\n",
       " 'station,': 353,\n",
       " 'rail': 354,\n",
       " 'mishap': 355,\n",
       " 'spurs': 356,\n",
       " 'large': 357,\n",
       " 'lasting': 358,\n",
       " 'headache': 359,\n",
       " 'chemical': 360,\n",
       " 'attack': 361,\n",
       " 'syrians': 362,\n",
       " 'ignites': 363,\n",
       " 'world’s': 364,\n",
       " 'outrage': 365,\n",
       " 'adventure': 366,\n",
       " 'still': 367,\n",
       " 'babbo’s': 368,\n",
       " 'menu': 369,\n",
       " 'swimming': 370,\n",
       " 'fast': 371,\n",
       " 'lane': 372,\n",
       " 'national': 373,\n",
       " 'civics': 374,\n",
       " 'exam': 375,\n",
       " 'obama': 376,\n",
       " 'adviser': 377,\n",
       " 'political': 378,\n",
       " 'cross': 379,\n",
       " 'hairs': 380,\n",
       " 'hippies': 381,\n",
       " 'won': 382,\n",
       " 'check': 383,\n",
       " 'box': 384,\n",
       " 'if': 385,\n",
       " 'you’re': 386,\n",
       " 'person': 387,\n",
       " 'couscous,': 388,\n",
       " 'chef’s': 389,\n",
       " 'patience': 390,\n",
       " 'pays': 391,\n",
       " 'three': 392,\n",
       " 'peas': 393,\n",
       " 'grain,': 394,\n",
       " 'playing': 395,\n",
       " 'well': 396,\n",
       " 'together': 397,\n",
       " 'fox': 398,\n",
       " 'like': 399,\n",
       " 'eagles?': 400,\n",
       " 'tired': 401,\n",
       " '‘hamilton’': 402,\n",
       " 'talk': 403,\n",
       " 'supreme': 404,\n",
       " 'court': 405,\n",
       " 'partisan': 406,\n",
       " 'tool': 407,\n",
       " '2': 408,\n",
       " 'picks': 409,\n",
       " 'education': 410,\n",
       " 'raise': 411,\n",
       " 'fears': 412,\n",
       " 'civil': 413,\n",
       " 'rights': 414,\n",
       " 'trump,': 415,\n",
       " 'focus': 416,\n",
       " 'u.s.': 417,\n",
       " 'interests': 418,\n",
       " 'disdain': 419,\n",
       " 'moralizing': 420,\n",
       " 'center': 421,\n",
       " 'universe': 422,\n",
       " '‘the': 423,\n",
       " 'americans’': 424,\n",
       " '5,': 425,\n",
       " '5': 426,\n",
       " 'recap:': 427,\n",
       " 'whole': 428,\n",
       " 'lotta': 429,\n",
       " 'shakin’': 430,\n",
       " 'badger': 431,\n",
       " 'cow': 432,\n",
       " 'jared': 433,\n",
       " 'kushner,': 434,\n",
       " 'man': 435,\n",
       " 'steel': 436,\n",
       " 'how-to': 437,\n",
       " 'book': 438,\n",
       " 'wielding': 439,\n",
       " 'civic': 440,\n",
       " 'power': 441,\n",
       " 'emperor': 442,\n",
       " 'real-world': 443,\n",
       " 'syria': 444,\n",
       " 'lesson': 445,\n",
       " '‘spacex': 446,\n",
       " 'launches': 447,\n",
       " 'satellite': 448,\n",
       " 'partly': 449,\n",
       " 'used': 450,\n",
       " 'rocket’': 451,\n",
       " 'ben': 452,\n",
       " 'sasse': 453,\n",
       " 'thinks': 454,\n",
       " 'biden': 455,\n",
       " 'would’ve': 456,\n",
       " 'earliest': 457,\n",
       " 'memory?': 458,\n",
       " 'secularist': 459,\n",
       " 'it': 460,\n",
       " 'o.k.': 461,\n",
       " 'our': 462,\n",
       " 'friends': 463,\n",
       " 'constantly': 464,\n",
       " 'suing': 465,\n",
       " 'people?': 466,\n",
       " 'diva': 467,\n",
       " 'departs': 468,\n",
       " 'takes': 469,\n",
       " 'suburb': 470,\n",
       " 'democratic': 471,\n",
       " 'turnout,': 472,\n",
       " 'low': 473,\n",
       " 'off-year': 474,\n",
       " 'races,': 475,\n",
       " 'appears': 476,\n",
       " 'rise': 477,\n",
       " 'mindful': 478,\n",
       " 'angry': 479,\n",
       " 'lessons': 480,\n",
       " 'mellow': 481,\n",
       " 'mice': 482,\n",
       " 'ask': 483,\n",
       " 'well;': 484,\n",
       " 'chewing': 485,\n",
       " 'gum': 486,\n",
       " 'toddlers?': 487,\n",
       " 'anyone?': 488,\n",
       " 'another': 489,\n",
       " 'thorny': 490,\n",
       " 'commute': 491,\n",
       " 'eighth': 492,\n",
       " 'annual': 493,\n",
       " 'found': 494,\n",
       " 'poem': 495,\n",
       " 'student': 496,\n",
       " 'contest': 497,\n",
       " 'women’s': 498,\n",
       " 'soccer': 499,\n",
       " 'team': 500,\n",
       " 'wins': 501,\n",
       " 'more,': 502,\n",
       " 'not': 503,\n",
       " 'equal,': 504,\n",
       " 'pay': 505,\n",
       " 'really': 506,\n",
       " 'security': 507,\n",
       " 'rationale': 508,\n",
       " 'banning': 509,\n",
       " 'laptops': 510,\n",
       " 'planes?': 511,\n",
       " 'seeking': 512,\n",
       " 'truths': 513,\n",
       " 'locked': 514,\n",
       " 'inside': 515,\n",
       " 'child': 516,\n",
       " 'soldier': 517,\n",
       " 'pepsi': 518,\n",
       " 'drops': 519,\n",
       " 'accused': 520,\n",
       " 'trivializing': 521,\n",
       " 'protesters': 522,\n",
       " 'eleven': 523,\n",
       " 'madison': 524,\n",
       " 'park': 525,\n",
       " 'tops': 526,\n",
       " 'list': 527,\n",
       " '50': 528,\n",
       " 'best': 529,\n",
       " 'restaurants': 530,\n",
       " 'bannon': 531,\n",
       " 'removed': 532,\n",
       " 'committee': 533,\n",
       " 'hawk': 534,\n",
       " 'soar': 535,\n",
       " 'suggests': 536,\n",
       " 'bigger': 537,\n",
       " 'role': 538,\n",
       " 'conflict': 539,\n",
       " 'cast': 540,\n",
       " 'doubt': 541,\n",
       " 'infrastructure': 542,\n",
       " 'groups': 543,\n",
       " 'seek': 544,\n",
       " 'court’s': 545,\n",
       " 'aid': 546,\n",
       " 'pesticide': 547,\n",
       " 'lobbying': 548,\n",
       " 'fierce': 549,\n",
       " '(and': 550,\n",
       " 'tasty)': 551,\n",
       " 'passing': 552,\n",
       " 'moe': 553,\n",
       " 'mr.': 554,\n",
       " 'most': 555,\n",
       " 'meeting': 556,\n",
       " 'dies,': 557,\n",
       " 'republicans': 558,\n",
       " 'can’t': 559,\n",
       " 'agree': 560,\n",
       " 'culprit': 561,\n",
       " 'fine': 562,\n",
       " 'romance?': 563,\n",
       " 'yes,': 564,\n",
       " 'gay': 565,\n",
       " 'history': 566,\n",
       " 'tour': 567,\n",
       " 'playbook': 568,\n",
       " 'symbols': 569,\n",
       " 'serving': 570,\n",
       " 'ham': 571,\n",
       " 'soignée': 572,\n",
       " 'silk': 573,\n",
       " 'fashion': 574,\n",
       " 'has': 575,\n",
       " 'covered': 576,\n",
       " 'only': 577,\n",
       " 'small': 578,\n",
       " 'army': 579,\n",
       " 'equal': 580,\n",
       " 'arnie': 581,\n",
       " 'public': 582,\n",
       " 'broadcasting': 583,\n",
       " 'learned': 584,\n",
       " 'younger': 585,\n",
       " '—': 586,\n",
       " 'taught': 587,\n",
       " 'person?': 588,\n",
       " 'i': 589,\n",
       " 'angered': 590,\n",
       " 'readers,': 591,\n",
       " 'again': 592,\n",
       " 'creeping': 593,\n",
       " 'toward': 594,\n",
       " 'crisis': 595,\n",
       " 'mistake': 596,\n",
       " 'war': 597,\n",
       " 'bets': 598,\n",
       " 'messy': 599,\n",
       " 'state': 600,\n",
       " 'u.s.-china': 601,\n",
       " 'ties:': 602,\n",
       " 'do': 603,\n",
       " '‘haley': 604,\n",
       " 'may': 605,\n",
       " '‘take': 606,\n",
       " 'own': 607,\n",
       " 'action’': 608,\n",
       " 'syrian': 609,\n",
       " 'attack’': 610,\n",
       " 'shadow': 611,\n",
       " 'fairy': 612,\n",
       " 'tale': 613,\n",
       " 'gut': 614,\n",
       " 'filibuster': 615,\n",
       " 'rule': 616,\n",
       " 'lift': 617,\n",
       " 'berliner': 618,\n",
       " 'fernsehturm': 619,\n",
       " 'improving': 620,\n",
       " 'original': 621,\n",
       " 'reader': 622,\n",
       " 'stories': 623,\n",
       " 'independence': 624,\n",
       " 'days': 625,\n",
       " 'snooping': 626,\n",
       " 'teenagers': 627,\n",
       " 'o.k.?': 628,\n",
       " 'city': 629,\n",
       " 'train': 630,\n",
       " 'donkey,': 631,\n",
       " 'find': 632,\n",
       " 'zebra': 633,\n",
       " 'facing': 634,\n",
       " 'scrutiny,': 635,\n",
       " 'intelligence': 636,\n",
       " 'chief': 637,\n",
       " 'leaves': 638,\n",
       " 'russia': 639,\n",
       " 'inquiry': 640,\n",
       " 'heart': 641,\n",
       " 'amazon': 642,\n",
       " '‘mrs.’': 643,\n",
       " 'became': 644,\n",
       " '‘ms.’': 645,\n",
       " 'recruiting': 646,\n",
       " 'assistants': 647,\n",
       " 'top': 648,\n",
       " 'job': 649,\n",
       " 'ewing': 650,\n",
       " 'dieting:': 651,\n",
       " 'yo-yo': 652,\n",
       " 'diets': 653,\n",
       " 'heart:': 654,\n",
       " 'race': 655,\n",
       " 'factor': 656,\n",
       " 'mixing': 657,\n",
       " 'drinks': 658,\n",
       " 'message': 659,\n",
       " 'simple': 660,\n",
       " 'taiwanese': 661,\n",
       " 'food': 662,\n",
       " 'doting': 663,\n",
       " 'mama': 664,\n",
       " 'don': 665,\n",
       " 'rickles,': 666,\n",
       " 'comedy’s': 667,\n",
       " 'opportunity': 668,\n",
       " 'offender,': 669,\n",
       " 'dies': 670,\n",
       " '90': 671,\n",
       " 'beaujolais,': 672,\n",
       " 'nouveau': 673,\n",
       " 'missiles': 674,\n",
       " 'feel': 675,\n",
       " 'american,': 676,\n",
       " 'try': 677,\n",
       " 'some': 678,\n",
       " 'velveeta': 679,\n",
       " 'fighting': 680,\n",
       " 'eviction,': 681,\n",
       " 'gardener': 682,\n",
       " 'turns': 683,\n",
       " 'organic': 684,\n",
       " 'industry': 685,\n",
       " 'giants': 686,\n",
       " 'help': 687,\n",
       " 'vaccines:': 688,\n",
       " 'moms’': 689,\n",
       " 'shot': 690,\n",
       " 'protects': 691,\n",
       " 'newborns': 692,\n",
       " 'without': 693,\n",
       " 'rikers': 694,\n",
       " 'island,': 695,\n",
       " 'learning': 696,\n",
       " 'love': 697,\n",
       " 'jail': 698,\n",
       " 'next': 699,\n",
       " 'door': 700,\n",
       " 'fingers': 701,\n",
       " 'crossed': 702,\n",
       " 'across': 703,\n",
       " 'generations': 704,\n",
       " 'donald': 705,\n",
       " 'bill': 706,\n",
       " 'roger': 707,\n",
       " 'ailes': 708,\n",
       " 'common?': 709,\n",
       " 'downsizing': 710,\n",
       " 'rock': 711,\n",
       " 'music': 712,\n",
       " 'sing': 713,\n",
       " 'song': 714,\n",
       " 'face': 715,\n",
       " 'creams': 716,\n",
       " 'after': 717,\n",
       " 'missiles,': 718,\n",
       " 'we': 719,\n",
       " 'need': 720,\n",
       " 'smart': 721,\n",
       " 'diplomacy': 722,\n",
       " 'youth,': 723,\n",
       " 'crowds,': 724,\n",
       " 'goals:': 725,\n",
       " 'germany': 726,\n",
       " 'aims': 727,\n",
       " 'keep': 728,\n",
       " 'way': 729,\n",
       " 'soul': 730,\n",
       " '…': 731,\n",
       " 'corporation': 732,\n",
       " 'coming': 733,\n",
       " 'incompetence': 734,\n",
       " 'bad,': 735,\n",
       " 'worse': 736,\n",
       " 'ugly': 737,\n",
       " 'optimists': 738,\n",
       " 'pessimists': 739,\n",
       " 'more': 740,\n",
       " 'journalists': 741,\n",
       " 'independent': 742,\n",
       " 'editorial': 743,\n",
       " 'control?': 744,\n",
       " 'ghost': 745,\n",
       " 'airbnb': 746,\n",
       " 'borrowers': 747,\n",
       " 'bewildered': 748,\n",
       " 'unemployment': 749,\n",
       " 'falls,': 750,\n",
       " 'feeble': 751,\n",
       " 'growth': 752,\n",
       " 'tempers': 753,\n",
       " 'optimism': 754,\n",
       " 'friday': 755,\n",
       " 'mailbag:': 756,\n",
       " 'senate': 757,\n",
       " 'votes': 758,\n",
       " 'museum': 759,\n",
       " 'drama': 760,\n",
       " 'teaching': 761,\n",
       " 'with:': 762,\n",
       " '‘animated': 763,\n",
       " 'life:': 764,\n",
       " 'mary': 765,\n",
       " 'leakey’': 766,\n",
       " 'muppet': 767,\n",
       " 'autism': 768,\n",
       " 'means': 769,\n",
       " 'preschool': 770,\n",
       " 'teachers': 771,\n",
       " 'graduates?': 772,\n",
       " 'no': 773,\n",
       " 'longer': 774,\n",
       " 'citi': 775,\n",
       " 'field’s': 776,\n",
       " 'alpha': 777,\n",
       " 'dog,': 778,\n",
       " 'harvey': 779,\n",
       " 'bite': 780,\n",
       " 'mexico': 781,\n",
       " 'outlaws': 782,\n",
       " 'school': 783,\n",
       " '‘lunch': 784,\n",
       " 'shaming’': 785,\n",
       " 'economic': 786,\n",
       " 'indicator?': 787,\n",
       " 'isn’t': 788,\n",
       " 'riddle': 789,\n",
       " 'strike': 790,\n",
       " 'air': 791,\n",
       " 'base': 792,\n",
       " 'angers': 793,\n",
       " 'russians': 794,\n",
       " 'nominee': 795,\n",
       " 'confirmed': 796,\n",
       " 'bruising': 797,\n",
       " 'yearlong': 798,\n",
       " 'fight': 799,\n",
       " 'disquieting': 800,\n",
       " 'silence': 801,\n",
       " 'near-zero': 802,\n",
       " 'interest': 803,\n",
       " 'rates:': 804,\n",
       " 'get': 805,\n",
       " 'them': 806,\n",
       " 'slice': 807,\n",
       " 'ambrosia': 808,\n",
       " 'filling)': 809,\n",
       " 'survived': 810,\n",
       " 'sarin': 811,\n",
       " 'gas': 812,\n",
       " 'first': 813,\n",
       " 'televised': 814,\n",
       " 'airstrikes,': 815,\n",
       " 'next?': 816,\n",
       " 'romance,': 817,\n",
       " 'sarcasm,': 818,\n",
       " 'math': 819,\n",
       " 'language': 820,\n",
       " 'who': 821,\n",
       " 'put': 822,\n",
       " 'those': 823,\n",
       " 'endless': 824,\n",
       " 'tunes': 825,\n",
       " 'fumes?': 826,\n",
       " 'share': 827,\n",
       " 'pta': 828,\n",
       " 'aid?': 829,\n",
       " 'parents': 830,\n",
       " 'would': 831,\n",
       " 'rather': 832,\n",
       " 'split': 833,\n",
       " 'district': 834,\n",
       " 'election': 835,\n",
       " 'left': 836,\n",
       " 'over': 837,\n",
       " 'economy': 838,\n",
       " 'marching': 839,\n",
       " 'bands': 840,\n",
       " 'fuels': 841,\n",
       " 'uncertainty': 842,\n",
       " 'ground': 843,\n",
       " 'clinton,': 844,\n",
       " 'free': 845,\n",
       " 'speak': 846,\n",
       " 'her': 847,\n",
       " 'mind': 848,\n",
       " 'passion': 849,\n",
       " 'southern': 850,\n",
       " 'christians': 851,\n",
       " 'wall': 852,\n",
       " 'happened': 853,\n",
       " 'who?': 854,\n",
       " 'president’s': 855,\n",
       " 'generals': 856,\n",
       " 'myth': 857,\n",
       " 'main': 858,\n",
       " 'street': 859,\n",
       " 'interviews': 860,\n",
       " 'offbeat': 861,\n",
       " 'approach': 862,\n",
       " 'covering': 863,\n",
       " 'sports': 864,\n",
       " 'having': 865,\n",
       " 'nothing': 866,\n",
       " 'getting': 867,\n",
       " 'canary': 868,\n",
       " 'tillerson': 869,\n",
       " 'halts': 870,\n",
       " 'any': 871,\n",
       " 'thawing': 872,\n",
       " 'ties': 873,\n",
       " 'words': 874,\n",
       " 'self-empowerment': 875,\n",
       " 'claiming': 876,\n",
       " 'odyssey’s': 877,\n",
       " 'reward': 878,\n",
       " 'familiar': 879,\n",
       " 'signature': 880,\n",
       " 'suits': 881,\n",
       " 'say': 882,\n",
       " 'lender': 883,\n",
       " 'duped': 884,\n",
       " 'students': 885,\n",
       " 'fuel': 886,\n",
       " 'station': 887,\n",
       " 'tie-ups': 888,\n",
       " 'depend': 889,\n",
       " 'knot': 890,\n",
       " 'agencies': 891,\n",
       " 'same': 892,\n",
       " 'old': 893,\n",
       " 'sergio': 894,\n",
       " 'delights': 895,\n",
       " 'crowd': 896,\n",
       " 'script': 897,\n",
       " '8:': 898,\n",
       " 'money,': 899,\n",
       " 'rules': 900,\n",
       " '6': 901,\n",
       " 'midnight': 902,\n",
       " 'descending': 903,\n",
       " 'spring': 904,\n",
       " 'break': 905,\n",
       " 'network': 906,\n",
       " '12:': 907,\n",
       " 'finale,': 908,\n",
       " 'carrie': 909,\n",
       " 'deals': 910,\n",
       " 'death': 911,\n",
       " 'betrayal': 912,\n",
       " 'bo': 913,\n",
       " 'diddley': 914,\n",
       " 'buddha?': 915,\n",
       " 'gig': 916,\n",
       " 'economy’s': 917,\n",
       " 'false': 918,\n",
       " 'promise': 919,\n",
       " 'publicity': 920,\n",
       " 'stunts': 921,\n",
       " 'aren’t': 922,\n",
       " 'policy': 923,\n",
       " 'road': 924,\n",
       " 'weapon': 925,\n",
       " 'alabama': 926,\n",
       " 'resigns': 927,\n",
       " 'pleads': 928,\n",
       " 'guilty': 929,\n",
       " 'amid': 930,\n",
       " 'sex': 931,\n",
       " 'scandal': 932,\n",
       " 'california': 933,\n",
       " 'moves': 934,\n",
       " 'protections': 935,\n",
       " 'immigrants;': 936,\n",
       " 'states': 937,\n",
       " 'follow': 938,\n",
       " 'he': 939,\n",
       " 'led': 940,\n",
       " 'yankees': 941,\n",
       " '4': 942,\n",
       " 'titles.': 943,\n",
       " 'revive': 944,\n",
       " 'them?': 945,\n",
       " 'fox:': 946,\n",
       " 'women': 947,\n",
       " 'often': 948,\n",
       " 'report': 949,\n",
       " 'sexual': 950,\n",
       " 'harassment': 951,\n",
       " 'work': 952,\n",
       " 'parents’': 953,\n",
       " 'mistakes': 954,\n",
       " 'rude': 955,\n",
       " 'doctors,': 956,\n",
       " 'nurses,': 957,\n",
       " 'patients': 958,\n",
       " 'passover,': 959,\n",
       " 'everyday': 960,\n",
       " 'plagues': 961,\n",
       " 'many': 962,\n",
       " 'pills': 963,\n",
       " 'too': 964,\n",
       " 'many?': 965,\n",
       " 'greatest': 966,\n",
       " 'earth': 967,\n",
       " 'wells': 968,\n",
       " 'fargo': 969,\n",
       " 'ex-leaders': 970,\n",
       " 'owe': 971,\n",
       " '$75': 972,\n",
       " 'million': 973,\n",
       " 'saved': 974,\n",
       " 'patients,': 975,\n",
       " 'furious': 976,\n",
       " 'families': 977,\n",
       " 'russian': 978,\n",
       " '‘boris': 979,\n",
       " 'badenov’': 980,\n",
       " 'high': 981,\n",
       " 'highlight': 982,\n",
       " 'president,': 983,\n",
       " 'sworn': 984,\n",
       " '113th': 985,\n",
       " 'deployment': 986,\n",
       " 'carrier': 987,\n",
       " 'masks': 988,\n",
       " 'lack': 989,\n",
       " 'better': 990,\n",
       " 'options': 991,\n",
       " 'dragged': 992,\n",
       " 'full': 993,\n",
       " 'jet,': 994,\n",
       " 'stirring': 995,\n",
       " 'furor': 996,\n",
       " '3': 997,\n",
       " 'pulitzers;': 998,\n",
       " 'service': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(article_lists[0])\n",
    "cleaned_sentence = [doc.lower() for doc in df.headline.values if doc not in string.punctuation ]\n",
    "\n",
    "\n",
    "# 모든 문장의 단어를 추출해 고유 번호 지정\n",
    "bow = {}\n",
    "for line in cleaned_sentence:\n",
    "    for w in line.split():\n",
    "        if w not in bow:\n",
    "            bow[w] = len(bow.keys())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "108c8ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 29, 30, 31]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bow[w] for w in cleaned_sentence[5].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ea089db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# kaggle data download\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "\n",
    "# csv파일이 있는 경로 path\n",
    "csv_lists = glob(path+'/*.*')\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    def __init__(self,csv_lists):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인의 텍스트를 불러옴\n",
    "        for filename in csv_lists:\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값은 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "        \n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
    "                                        for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  # 입력 데이터\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ac8ca76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array(2., dtype=float32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextGeneration(csv_lists)\n",
    "x, y = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c4a7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):  # num_embeddings 전체 단어의 개수(어휘사전 크기)\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # 신경망이 이해할수 있도록 벡터로 변경\n",
    "        self.embed = nn.Embedding( num_embeddings=num_embeddings, embedding_dim=16)\n",
    "\n",
    "        # LSTM을 5개층  (배치, 시퀀스, 피처)  16 ~ 512\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=64, num_layers=5,batch_first=True)\n",
    "        # 분류를위한 fc층  squence_length * 64 = 2*64  128\n",
    "        self.fc1 = nn.Linear( 128, num_embeddings)\n",
    "        self.fc2 = nn.Linear( num_embeddings , num_embeddings)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):  # 입력 (batch , sq_len) batch: 32  sq_len : 2\n",
    "        x = self.embed(x)  # 출력 (batch, sq_len, 16)  32 2 16\n",
    "\n",
    "        # lstm 모델 예측값\n",
    "        x, _ =  self.lstm(x) # 출력 (batch ,sq_len, 64)  32 2 64\n",
    "        x = torch.reshape(x, (x.shape[0], -1))  # 출력 (batch, sq_len x 64)  32, 128\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "90708c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2482"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c73b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]C:\\Users\\sally\\AppData\\Local\\Temp\\ipykernel_31996\\3195842733.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long))\n",
      "C:\\Users\\sally\\AppData\\Local\\Temp\\ipykernel_31996\\3195842733.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n",
      "epoch:1 loss : 7.3657402992248535: 100%|██████████| 125/125 [00:08<00:00, 14.71it/s]\n",
      "epoch:2 loss : 7.053006172180176: 100%|██████████| 125/125 [00:08<00:00, 14.68it/s] \n",
      "epoch:3 loss : 6.412163734436035: 100%|██████████| 125/125 [00:08<00:00, 14.20it/s] \n",
      "epoch:4 loss : 5.960381507873535: 100%|██████████| 125/125 [00:12<00:00, 10.26it/s]\n",
      "epoch:5 loss : 5.807261943817139: 100%|██████████| 125/125 [00:11<00:00, 10.78it/s] \n",
      "epoch:6 loss : 5.7566704750061035: 100%|██████████| 125/125 [00:07<00:00, 16.86it/s]\n",
      "epoch:7 loss : 5.624445915222168: 100%|██████████| 125/125 [00:08<00:00, 15.20it/s] \n",
      "epoch:8 loss : 5.799221992492676: 100%|██████████| 125/125 [00:10<00:00, 11.76it/s] \n",
      "epoch:9 loss : 5.693027496337891: 100%|██████████| 125/125 [00:14<00:00,  8.57it/s] \n",
      "epoch:10 loss : 5.77903413772583: 100%|██████████| 125/125 [00:19<00:00,  6.44it/s]  \n",
      "epoch:11 loss : 5.984919548034668: 100%|██████████| 125/125 [00:15<00:00,  8.28it/s] \n",
      "epoch:12 loss : 5.794815540313721: 100%|██████████| 125/125 [00:19<00:00,  6.27it/s] \n",
      "epoch:13 loss : 5.686729907989502: 100%|██████████| 125/125 [00:37<00:00,  3.36it/s] \n",
      "epoch:14 loss : 5.700526237487793: 100%|██████████| 125/125 [00:34<00:00,  3.63it/s] \n",
      "epoch:15 loss : 5.872218132019043: 100%|██████████| 125/125 [01:27<00:00,  1.43it/s] \n",
      "epoch:16 loss : 5.537787437438965: 100%|██████████| 125/125 [02:03<00:00,  1.02it/s] \n",
      "epoch:17 loss : 5.6863484382629395: 100%|██████████| 125/125 [01:30<00:00,  1.38it/s]\n",
      "epoch:18 loss : 5.609727382659912: 100%|██████████| 125/125 [00:48<00:00,  2.59it/s] \n",
      "epoch:19 loss : 5.629158020019531: 100%|██████████| 125/125 [00:29<00:00,  4.29it/s] \n",
      "epoch:20 loss : 5.578401565551758: 100%|██████████| 125/125 [00:18<00:00,  6.60it/s] \n",
      "epoch:21 loss : 5.65880823135376: 100%|██████████| 125/125 [00:31<00:00,  3.91it/s]  \n",
      "epoch:22 loss : 5.6481614112854: 100%|██████████| 125/125 [01:09<00:00,  1.80it/s]   \n",
      "epoch:23 loss : 6.135298728942871: 100%|██████████| 125/125 [00:17<00:00,  7.26it/s] \n",
      "epoch:24 loss : 5.629680156707764: 100%|██████████| 125/125 [00:17<00:00,  7.34it/s] \n",
      "epoch:25 loss : 5.625909328460693: 100%|██████████| 125/125 [00:21<00:00,  5.69it/s] \n",
      "epoch:26 loss : 5.714504241943359: 100%|██████████| 125/125 [00:26<00:00,  4.65it/s] \n",
      "epoch:27 loss : 5.6548662185668945: 100%|██████████| 125/125 [01:04<00:00,  1.93it/s]\n",
      "epoch:28 loss : 5.6863694190979: 100%|██████████| 125/125 [00:28<00:00,  4.32it/s]   \n",
      "epoch:29 loss : 5.544089317321777: 100%|██████████| 125/125 [00:13<00:00,  9.08it/s] \n",
      "epoch:30 loss : 5.554690837860107: 100%|██████████| 125/125 [00:15<00:00,  8.21it/s] \n",
      "epoch:31 loss : 5.655412197113037: 100%|██████████| 125/125 [00:15<00:00,  7.89it/s] \n",
      "epoch:32 loss : 5.60880708694458: 100%|██████████| 125/125 [00:25<00:00,  4.88it/s]  \n",
      "epoch:33 loss : 5.631862163543701: 100%|██████████| 125/125 [00:32<00:00,  3.84it/s] \n",
      "epoch:34 loss : 5.263001918792725: 100%|██████████| 125/125 [01:31<00:00,  1.36it/s] \n",
      "epoch:35 loss : 5.417096138000488: 100%|██████████| 125/125 [00:39<00:00,  3.13it/s] \n",
      "epoch:36 loss : 5.992795467376709: 100%|██████████| 125/125 [01:14<00:00,  1.68it/s]\n",
      "epoch:37 loss : 6.263546466827393: 100%|██████████| 125/125 [01:28<00:00,  1.41it/s] \n",
      "epoch:38 loss : 6.282366752624512: 100%|██████████| 125/125 [00:12<00:00,  9.62it/s] \n",
      "epoch:39 loss : 6.231716156005859: 100%|██████████| 125/125 [00:06<00:00, 19.42it/s] \n",
      "epoch:40 loss : 6.119111061096191: 100%|██████████| 125/125 [00:06<00:00, 19.26it/s]\n",
      "epoch:41 loss : 6.102324962615967: 100%|██████████| 125/125 [00:06<00:00, 19.21it/s] \n",
      "epoch:42 loss : 5.5224928855896: 100%|██████████| 125/125 [00:06<00:00, 19.55it/s]   \n",
      "epoch:43 loss : 5.491583824157715: 100%|██████████| 125/125 [00:06<00:00, 19.39it/s] \n",
      "epoch:44 loss : 5.553479194641113: 100%|██████████| 125/125 [00:06<00:00, 19.52it/s] \n",
      "epoch:45 loss : 5.714023113250732: 100%|██████████| 125/125 [00:06<00:00, 19.58it/s] \n",
      "epoch:46 loss : 5.372926712036133: 100%|██████████| 125/125 [00:06<00:00, 19.68it/s] \n",
      "epoch:47 loss : 5.4165825843811035: 100%|██████████| 125/125 [00:06<00:00, 19.74it/s]\n",
      "epoch:48 loss : 5.7305073738098145: 100%|██████████| 125/125 [00:06<00:00, 19.02it/s]\n",
      "epoch:49 loss : 5.61946439743042: 100%|██████████| 125/125 [00:06<00:00, 18.70it/s]  \n",
      "epoch:50 loss : 5.683873653411865: 100%|██████████| 125/125 [00:06<00:00, 18.59it/s] \n",
      "epoch:51 loss : 5.694613456726074: 100%|██████████| 125/125 [00:06<00:00, 19.41it/s] \n",
      "epoch:52 loss : 5.677161693572998: 100%|██████████| 125/125 [00:06<00:00, 19.06it/s] \n",
      "epoch:53 loss : 5.725734710693359: 100%|██████████| 125/125 [00:06<00:00, 19.66it/s] \n",
      "epoch:54 loss : 5.664648532867432: 100%|██████████| 125/125 [00:06<00:00, 18.84it/s] \n",
      "epoch:55 loss : 5.623161315917969: 100%|██████████| 125/125 [00:06<00:00, 18.66it/s] \n",
      "epoch:56 loss : 5.58475923538208: 100%|██████████| 125/125 [00:06<00:00, 19.54it/s]  \n",
      "epoch:57 loss : 5.625714302062988: 100%|██████████| 125/125 [00:06<00:00, 19.55it/s] \n",
      "epoch:58 loss : 5.542445659637451: 100%|██████████| 125/125 [00:06<00:00, 19.72it/s] \n",
      "epoch:59 loss : 5.675244331359863: 100%|██████████| 125/125 [00:06<00:00, 18.94it/s] \n",
      "epoch:60 loss : 5.507497310638428: 100%|██████████| 125/125 [00:06<00:00, 19.27it/s] \n",
      "epoch:61 loss : 5.448342800140381: 100%|██████████| 125/125 [00:06<00:00, 18.70it/s] \n",
      "epoch:62 loss : 5.3708295822143555: 100%|██████████| 125/125 [00:06<00:00, 18.24it/s]\n",
      "epoch:63 loss : 5.3921098709106445: 100%|██████████| 125/125 [00:06<00:00, 18.92it/s]\n",
      "epoch:64 loss : 5.391661167144775: 100%|██████████| 125/125 [00:06<00:00, 19.87it/s] \n",
      "epoch:65 loss : 5.331969261169434: 100%|██████████| 125/125 [00:06<00:00, 19.80it/s] \n",
      "epoch:66 loss : 5.275771141052246: 100%|██████████| 125/125 [00:06<00:00, 19.87it/s] \n",
      "epoch:67 loss : 5.3010687828063965: 100%|██████████| 125/125 [00:06<00:00, 19.71it/s]\n",
      "epoch:68 loss : 5.341331481933594: 100%|██████████| 125/125 [00:06<00:00, 20.07it/s] \n",
      "epoch:69 loss : 5.3227009773254395: 100%|██████████| 125/125 [00:06<00:00, 19.76it/s]\n",
      "epoch:70 loss : 5.278797626495361: 100%|██████████| 125/125 [00:06<00:00, 19.75it/s] \n",
      "epoch:71 loss : 5.331418037414551: 100%|██████████| 125/125 [00:06<00:00, 18.81it/s] \n",
      "epoch:72 loss : 5.322024345397949: 100%|██████████| 125/125 [00:07<00:00, 16.79it/s] \n",
      "epoch:73 loss : 5.408247470855713: 100%|██████████| 125/125 [00:07<00:00, 17.35it/s]\n",
      "epoch:74 loss : 5.334828853607178: 100%|██████████| 125/125 [00:08<00:00, 14.67it/s] \n",
      "epoch:75 loss : 5.221718788146973: 100%|██████████| 125/125 [00:07<00:00, 16.19it/s] \n",
      "epoch:76 loss : 5.199032783508301: 100%|██████████| 125/125 [00:08<00:00, 14.96it/s] \n",
      "epoch:77 loss : 5.198042869567871: 100%|██████████| 125/125 [00:08<00:00, 15.22it/s]\n",
      "epoch:78 loss : 5.090054512023926: 100%|██████████| 125/125 [00:08<00:00, 14.96it/s] \n",
      "epoch:79 loss : 5.105592727661133: 100%|██████████| 125/125 [00:08<00:00, 14.00it/s]\n",
      "epoch:80 loss : 5.066369533538818: 100%|██████████| 125/125 [00:07<00:00, 16.31it/s] \n",
      "epoch:81 loss : 5.081519603729248: 100%|██████████| 125/125 [00:07<00:00, 16.47it/s] \n",
      "epoch:82 loss : 5.0847673416137695: 100%|██████████| 125/125 [00:08<00:00, 14.22it/s]\n",
      "epoch:83 loss : 5.001521110534668: 100%|██████████| 125/125 [00:07<00:00, 17.82it/s] \n",
      "epoch:84 loss : 5.036740779876709: 100%|██████████| 125/125 [00:07<00:00, 16.48it/s] \n",
      "epoch:85 loss : 5.0251264572143555: 100%|██████████| 125/125 [00:07<00:00, 16.18it/s]\n",
      "epoch:86 loss : 5.065423011779785: 100%|██████████| 125/125 [00:07<00:00, 15.79it/s] \n",
      "epoch:87 loss : 4.905984401702881: 100%|██████████| 125/125 [00:07<00:00, 16.10it/s] \n",
      "epoch:88 loss : 4.874091625213623: 100%|██████████| 125/125 [00:07<00:00, 16.83it/s] \n",
      "epoch:89 loss : 4.900382041931152: 100%|██████████| 125/125 [00:09<00:00, 13.65it/s] \n",
      "epoch:90 loss : 4.851265907287598: 100%|██████████| 125/125 [00:08<00:00, 14.46it/s] \n",
      "epoch:91 loss : 4.8528032302856445: 100%|██████████| 125/125 [00:07<00:00, 17.65it/s]\n",
      "epoch:92 loss : 4.883315086364746: 100%|██████████| 125/125 [00:07<00:00, 17.69it/s] \n",
      "epoch:93 loss : 4.785598278045654: 100%|██████████| 125/125 [00:07<00:00, 17.50it/s] \n",
      "epoch:94 loss : 4.809752464294434: 100%|██████████| 125/125 [00:07<00:00, 15.67it/s] \n",
      "epoch:95 loss : 4.694558620452881: 100%|██████████| 125/125 [00:07<00:00, 16.70it/s] \n",
      "epoch:96 loss : 4.780883312225342: 100%|██████████| 125/125 [00:06<00:00, 17.97it/s] \n",
      "epoch:97 loss : 4.8598527908325195: 100%|██████████| 125/125 [00:07<00:00, 16.69it/s]\n",
      "epoch:98 loss : 4.967200756072998: 100%|██████████| 125/125 [00:08<00:00, 14.81it/s] \n",
      "epoch:99 loss : 4.862556457519531: 100%|██████████| 125/125 [00:06<00:00, 17.98it/s] \n",
      "epoch:100 loss : 4.846253395080566: 100%|██████████| 125/125 [00:07<00:00, 16.31it/s] \n",
      "epoch:101 loss : 4.790860652923584: 100%|██████████| 125/125 [00:07<00:00, 16.83it/s] \n",
      "epoch:102 loss : 4.791129112243652: 100%|██████████| 125/125 [00:06<00:00, 17.90it/s] \n",
      "epoch:103 loss : 4.816616535186768: 100%|██████████| 125/125 [00:08<00:00, 14.53it/s] \n",
      "epoch:104 loss : 4.748824119567871: 100%|██████████| 125/125 [00:07<00:00, 16.65it/s] \n",
      "epoch:105 loss : 4.69805908203125: 100%|██████████| 125/125 [00:07<00:00, 16.72it/s]  \n",
      "epoch:106 loss : 4.682493209838867: 100%|██████████| 125/125 [00:07<00:00, 17.09it/s] \n",
      "epoch:107 loss : 4.8916850090026855: 100%|██████████| 125/125 [00:07<00:00, 17.56it/s]\n",
      "epoch:108 loss : 4.726996421813965: 100%|██████████| 125/125 [00:07<00:00, 15.78it/s] \n",
      "epoch:109 loss : 4.645152568817139: 100%|██████████| 125/125 [00:08<00:00, 15.30it/s] \n",
      "epoch:110 loss : 4.6105055809021: 100%|██████████| 125/125 [00:07<00:00, 17.01it/s]   \n",
      "epoch:111 loss : 4.578534126281738: 100%|██████████| 125/125 [00:07<00:00, 17.29it/s] \n",
      "epoch:112 loss : 4.562678337097168: 100%|██████████| 125/125 [00:07<00:00, 15.95it/s] \n",
      "epoch:113 loss : 4.527724266052246: 100%|██████████| 125/125 [00:07<00:00, 16.61it/s] \n",
      "epoch:114 loss : 4.539052963256836: 100%|██████████| 125/125 [00:07<00:00, 16.29it/s] \n",
      "epoch:115 loss : 4.516203880310059: 100%|██████████| 125/125 [00:07<00:00, 16.12it/s] \n",
      "epoch:116 loss : 4.39246940612793: 100%|██████████| 125/125 [00:08<00:00, 15.54it/s]  \n",
      "epoch:117 loss : 4.43388032913208: 100%|██████████| 125/125 [00:07<00:00, 16.69it/s]  \n",
      "epoch:118 loss : 4.346249580383301: 100%|██████████| 125/125 [00:07<00:00, 16.24it/s] \n",
      "epoch:119 loss : 4.497067451477051: 100%|██████████| 125/125 [00:07<00:00, 16.79it/s] \n",
      "epoch:120 loss : 4.4545159339904785: 100%|██████████| 125/125 [00:08<00:00, 15.03it/s]\n",
      "epoch:121 loss : 4.363491535186768: 100%|██████████| 125/125 [00:08<00:00, 14.10it/s] \n",
      "epoch:122 loss : 4.3885498046875: 100%|██████████| 125/125 [00:08<00:00, 14.76it/s]   \n",
      "epoch:123 loss : 4.39016580581665: 100%|██████████| 125/125 [27:34<00:00, 13.24s/it]    \n",
      "epoch:124 loss : 4.415012359619141: 100%|██████████| 125/125 [14:49:47<00:00, 427.10s/it]     \n",
      "epoch:125 loss : 4.605134963989258: 100%|██████████| 125/125 [00:07<00:00, 15.97it/s] \n",
      "epoch:126 loss : 4.575928688049316: 100%|██████████| 125/125 [00:08<00:00, 15.27it/s] \n",
      "epoch:127 loss : 4.594176292419434: 100%|██████████| 125/125 [00:08<00:00, 14.07it/s] \n",
      "epoch:128 loss : 4.515258312225342: 100%|██████████| 125/125 [00:08<00:00, 15.17it/s] \n",
      "epoch:129 loss : 4.4847092628479: 100%|██████████| 125/125 [00:11<00:00, 10.76it/s]   \n",
      "epoch:130 loss : 4.381555557250977: 100%|██████████| 125/125 [00:16<00:00,  7.41it/s] \n",
      "epoch:131 loss : 4.3091254234313965: 100%|██████████| 125/125 [00:10<00:00, 11.51it/s]\n",
      "epoch:132 loss : 4.27980899810791: 100%|██████████| 125/125 [00:10<00:00, 12.37it/s]  \n",
      "epoch:133 loss : 4.399679183959961: 100%|██████████| 125/125 [00:09<00:00, 12.52it/s] \n",
      "epoch:134 loss : 4.597311496734619: 100%|██████████| 125/125 [00:11<00:00, 11.21it/s] \n",
      "epoch:135 loss : 4.4300456047058105: 100%|██████████| 125/125 [00:10<00:00, 11.51it/s]\n",
      "epoch:136 loss : 4.426905155181885: 100%|██████████| 125/125 [00:17<00:00,  7.25it/s] \n",
      "epoch:137 loss : 4.442076206207275: 100%|██████████| 125/125 [00:16<00:00,  7.51it/s] \n",
      "epoch:138 loss : 4.285738945007324: 100%|██████████| 125/125 [00:16<00:00,  7.51it/s] \n",
      "epoch:139 loss : 4.290070056915283: 100%|██████████| 125/125 [00:16<00:00,  7.71it/s] \n",
      "epoch:140 loss : 4.15531063079834: 100%|██████████| 125/125 [00:10<00:00, 11.52it/s]  \n",
      "epoch:141 loss : 4.100405216217041: 100%|██████████| 125/125 [00:11<00:00, 11.01it/s] \n",
      "epoch:142 loss : 4.008026123046875: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s] \n",
      "epoch:143 loss : 4.121766567230225: 100%|██████████| 125/125 [00:10<00:00, 12.03it/s] \n",
      "epoch:144 loss : 4.051316738128662: 100%|██████████| 125/125 [00:12<00:00, 10.39it/s] \n",
      "epoch:145 loss : 4.238869667053223: 100%|██████████| 125/125 [00:17<00:00,  7.21it/s] \n",
      "epoch:146 loss : 3.9848122596740723: 100%|██████████| 125/125 [00:23<00:00,  5.26it/s]\n",
      "epoch:147 loss : 4.307130336761475: 100%|██████████| 125/125 [00:22<00:00,  5.51it/s] \n",
      "epoch:148 loss : 4.026545524597168: 100%|██████████| 125/125 [00:18<00:00,  6.92it/s] \n",
      "epoch:149 loss : 4.121659278869629: 100%|██████████| 125/125 [01:09<00:00,  1.80it/s] \n",
      "epoch:150 loss : 3.914994716644287: 100%|██████████| 125/125 [01:14<00:00,  1.69it/s] \n",
      "epoch:151 loss : 3.8180649280548096: 100%|██████████| 125/125 [00:55<00:00,  2.24it/s]\n",
      "epoch:152 loss : 3.9328231811523438: 100%|██████████| 125/125 [00:51<00:00,  2.44it/s]\n",
      "epoch:153 loss : 3.9023678302764893: 100%|██████████| 125/125 [00:15<00:00,  7.81it/s]\n",
      "epoch:154 loss : 3.8024864196777344: 100%|██████████| 125/125 [00:15<00:00,  8.14it/s]\n",
      "epoch:155 loss : 3.788248062133789: 100%|██████████| 125/125 [00:18<00:00,  6.83it/s] \n",
      "epoch:156 loss : 4.48048210144043:  63%|██████▎   | 79/125 [00:11<00:06,  7.11it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n\u001b[32m     18\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loop.set_description(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mepoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\adam.py:519\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    517\u001b[39m         param.addcdiv_(exp_avg, denom)\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     step = \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     bias_correction1 = \u001b[32m1\u001b[39m - beta1**step\n\u001b[32m    522\u001b[39m     bias_correction2 = \u001b[32m1\u001b[39m - beta2**step\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\optim\\optimizer.py:97\u001b[39m, in \u001b[36m_get_value\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 모델....\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = TextGeneration(csv_lists)\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset,batch_size=32)\n",
    "optim = Adam(model.parameters(), lr = 1e-3)\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        data,label = data.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data, dtype=torch.long))\n",
    "        loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'epoch:{epoch+1} loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 예측\n",
    "# 입력문장을 텐서로 변경, 임베딩 벡터 BOW를 이용해서\n",
    "sample = \"i love\"\n",
    "with torch.no_grad():\n",
    "    words = torch.tensor(\n",
    "        [dataset.BOW[w] for w in sample.splot()],dtype=torch.long\n",
    "    ).to(device).unsqueeze(0)\n",
    "    output = model(words)\n",
    "    # 출력은 단어의 갯수만큼 len(BOW) (batch, len(BOW))\n",
    "    # 확률이 가장 높은 단어기\n",
    "    predicted_index = torch.argmax(output,dim=1).item()\n",
    "    # 단어사전 BOW에서 인덱스에 해당하는 단어를 찾기\n",
    "    # 역 dict를 만들어서 찾기\n",
    "    reverse_bow = {value:key for key,value in dataset.BOW.items()}\n",
    "    predicted_word = reverse_bow[predicted_index]\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
