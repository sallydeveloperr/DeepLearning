{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5d6494",
   "metadata": {},
   "source": [
    "##### DNN CNN\n",
    "    - 독립적인 정보\n",
    "    - 입력(x)간의 순서나 연관성을 고려하지 않는다\n",
    "##### 시계열 : 시간의 연속적인 흐름\n",
    "    - 시계열 데이터 : 날씨, 주식, 문장 --> 순서가 중요한 데이터\n",
    "##### RNN\n",
    "    - 순환하는 구조\n",
    "        - 시점1(월요일) : 맑음(x1) 정보가 RNN에 들어온다 -> RNN 날씨가 맑았음(h1) 이라는 요약본을 생성\n",
    "        - 시점2(화요일) : 흐림(x2) -> RNN 새로운 정보(흐림, x2) + 어제의 기억(맑았음, h1) 함께 고려해서 어제 맑았는데 오늘 흐림 h2라는 새로운 요약본을 생성\n",
    "        - 시점3(수요일) : 비(x3) -> RNN 새로운 정보(비, x3)와 어제의 기억(h2)을 함께 고려해서\n",
    "        새로운 상태 h3\n",
    "        - 계속 반복\n",
    "    - 알고리즘\n",
    "        - 각 시점(time step)에서 1. 현재의 입력과 2. 과거의 기억(hidden state ht-1) 받아서 3.3. 현재의 결과물과 4. 다음 시점으로 넘겨줄 최신기억 ht를 생성\n",
    "        - ht 기억이 시계열 데이터의 맥락(context)을 저장하는 역할\n",
    "    - 장점\n",
    "        - 순서가 있는 데이터의 맥락을 학습한다.\n",
    "        - 입출력의 길이에 유연하게 대처한다.\n",
    "    - 한계\n",
    "        - 기억력이 생각보다 짧다.\n",
    "        - 시계열 데이터가 길어지면(예. 100단계 전의 정보)\n",
    "        - 이전 정보가 소실(Vanishing Gradient)되거나 반대로 너무 강해져서 폭주(Exploding Gradient)가 돼서 제대로 학습이 안된다.\n",
    "        - 장기 기억 의존성 문제(long-term dependency problem)\n",
    "##### LSTM & GRU\n",
    "    - LSTM(Long Short-Term Memory) : RNN 내부에 게이트(Gate)라는 복잡한 장치 -> 잊고 기억할 정보를 관리\n",
    "    - GRU(Gated Recurrent Unit) : LSTM 구조를 좀 더 단순화시킨 모델, LSTM 성능은 비슷, 속도는 빠르다.\n",
    "##### RNN 핵심수식\n",
    "    - 은닉 상태 계산\n",
    "        - h1 = tanh(wxht-1 + wxt - bh)\n",
    "    - 출력계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b3145a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>120</td>\n",
       "      <td>123</td>\n",
       "      <td>118</td>\n",
       "      <td>13181000</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-17</td>\n",
       "      <td>124</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>17284900</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-18</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>118</td>\n",
       "      <td>17948100</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-21</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>11670000</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-22</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>115</td>\n",
       "      <td>9689000</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open  High  Low    Volume  Close\n",
       "0  2015-12-16   120   123  118  13181000    123\n",
       "1  2015-12-17   124   126  122  17284900    123\n",
       "2  2015-12-18   121   122  118  17948100    118\n",
       "3  2015-12-21   120   120  116  11670000    117\n",
       "4  2015-12-22   117   117  115   9689000    116"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/pia222sk20/python/refs/heads/main/data/time_data_train.csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9276f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 967 entries, 0 to 966\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    967 non-null    object\n",
      " 1   Open    967 non-null    int64 \n",
      " 2   High    967 non-null    int64 \n",
      " 3   Low     967 non-null    int64 \n",
      " 4   Volume  967 non-null    int64 \n",
      " 5   Close   967 non-null    int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 45.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "276ca82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d2f37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "class StockDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        url = 'https://raw.githubusercontent.com/pia222sk20/python/refs/heads/main/data/time_data_train.csv'\n",
    "        self.csv = pd.read_csv(url)\n",
    "        data = self.csv.iloc[:,1:-1].values\n",
    "        label = self.csv.iloc[:,-1].values.reshape(-1,1)\n",
    "        self.data = StandardScaler().fit_transform(data)\n",
    "        \n",
    "        # 정답이 숫자 크다면 정규화가 학습에 도움이 된다.\n",
    "        self.label = StandardScaler().fit_transform(label)\n",
    "        self.data = torch.Tensor(self.data)\n",
    "        self.label = torch.Tensor(self.label)\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 30  #사용가능한 배치 갯수\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index: index + 30]\n",
    "        label = self.label[index + 30]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a06bcd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 4]), torch.Size([1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = next(iter(StockDataSet()))\n",
    "data.size(), label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0897d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class StockRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=4, hidden_size=8, num_layers=5,batch_first=True)\n",
    "        self.fc1 = nn.Linear(30*8,64)\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x,ho):     #입력데이터는 (16,30,4)\n",
    "        # ho는 초기 은닉상태\n",
    "        # 출력 x는 모든 시점에 대한 hidden output을 담고 있어야..\n",
    "        # 출력 hn 최종은닉상태 (각 레이어의 마지막 타임 스탬프 hidden state)\n",
    "        x,hn = self.rnn(x,ho)\n",
    "        #mlp 입력으로 사용될 수 있도록 모양 변경\n",
    "        x = torch.reshape(x,(x.shape[0],-1))\n",
    "        #mlp\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #예측한 종가 1차원 벡터\n",
    "        out = torch.flatten(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a55e5fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = StockRNN()\n",
    "sample_data = torch.randn(16,30,4)\n",
    "# 초기 hidden state 값(num_layer, batch_size, hidden_size) (5,16,8)\n",
    "ho = torch.zeros(5,16,8)\n",
    "out = rnn(sample_data,ho)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a6ff6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = StockRNN().to(device)\n",
    "dataset = StockDataSet()\n",
    "loader = DataLoader(dataset, batch_size=16)\n",
    "optim = Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6019b7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data,label = next(iter(loader))\n",
    "data.dtype, label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f98c39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Expected hidden size (5, 9, 8), got [5, 16, 8]\n",
    "# Rnn이 처리하는 배치크기는 9  우리가 설계한 ho 16\n",
    "print(len(dataset) % 25)\n",
    "# 마지막 배치가 개수가 모라자서 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "epoch:1 loss:0.2942065894603729:  83%|████████▎ | 49/59 [00:00<00:00, 96.90it/s]   c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "epoch:1 loss:0.2537175714969635: 100%|██████████| 59/59 [00:00<00:00, 94.35it/s]\n",
      "epoch:2 loss:0.5475544929504395: 100%|██████████| 59/59 [00:00<00:00, 70.28it/s]  \n",
      "epoch:3 loss:0.30059173703193665: 100%|██████████| 59/59 [00:00<00:00, 63.76it/s]  \n",
      "epoch:4 loss:0.526803195476532: 100%|██████████| 59/59 [00:01<00:00, 52.72it/s]   \n",
      "epoch:5 loss:0.48098018765449524: 100%|██████████| 59/59 [00:08<00:00,  6.64it/s]  \n",
      "epoch:6 loss:0.4229528605937958: 100%|██████████| 59/59 [00:06<00:00,  9.54it/s]   \n",
      "epoch:7 loss:0.010360787622630596:   8%|▊         | 5/59 [00:01<00:14,  3.74it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# criterian = nn.MSELoss()\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        optim.zero_grad()\n",
    "        # 초기 은닉상태의 배치크기는 DataLoader가 주는 배치 크기\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        # 모델의 예측값\n",
    "        pred = model(data.to(device), ho)\n",
    "        # 손실값\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'epoch:{epoch+1} loss:{loss.item()}')\n",
    "torch.save(model.state_dict(), './rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 성능평가하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loader = DataLoader(dataset,batch_size=1)\n",
    "preds = []\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load('rnn.pth', map_location=device,weights_only=False))\n",
    "    for data, label in loader:\n",
    "        # 초기 은닉상태의 배치크기는 DataLoader가 주는 배치 크기\n",
    "        batch_size = data.size(0)\n",
    "\n",
    "        ho = torch.zeros(5,batch_size,8).to(device)\n",
    "        # 모델의 예측값\n",
    "        pred = model(data.to(device), ho)\n",
    "        preds.append(pred.item())\n",
    "        # 손실값\n",
    "        loss = nn.MSELoss()(pred, label.to(device))\n",
    "        total_loss += (loss.item() / len(loader))\n",
    "\n",
    "print(f'total_loss : {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(preds,label='preds')\n",
    "plt.plot(dataset.label[30:], label = 'real')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
