{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865aca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bow Bag of Words CountVectorizer\n",
    "# ë¬¸ì„œë¥¼ ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜\n",
    "# ë¬¸ì„œ - ë‹¨ì–´í–‰ë ¬\n",
    "# ì¥ì  : ê°„ë‹¨í•˜ê³  ë¹ ë¦„\n",
    "# ë‹¨ì  : ë‹¨ì–´ ìˆœì„œ ì†ì‹¤, í¬ì†Œì„±, ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¬´ì‹œ\n",
    "\n",
    "# tf-idf TfidfVectorizer\n",
    "# ëª¨ë“  ë¬¸ì„œì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ì˜í–¥ì„ ì¤„ì´ê³  ë¬¸ì„œ íŠ¹ì´ ë‹¨ì–´ë¥¼ ê°•ì¡°\n",
    "\n",
    "# multinomal Navie Bayes í™•ë¥  ëª¨ë¸\n",
    "# LogisticRegression ë‹¤ì¤‘ í´ë˜ìŠ¤ íšŒê·€ ê¸°ë°˜ ë¶„ë¥˜\n",
    "\n",
    "# RidgeClassifier íšŒê·€ ê¸°ë°˜ ë¶„ë¥˜ L2 ê·œì œ\n",
    "\n",
    "# N-gram ë‹¨ì  : ì°¨ì› í­ë°œì— ì£¼ì˜ (ì •ê·œí™”/ì°¨ì› ì¶•ì†Œ ê³ ë ¤)\n",
    "\n",
    "# Kolnpy Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21616df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# scikit-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ë¶„ë¥˜ëª¨ë¸\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ad93d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "newsgroup_train = fetch_20newsgroups(subset='train',\n",
    "                remove=('headers','footers','quotes'),\n",
    "                categories=categories\n",
    "                )\n",
    "newsgroup_test = fetch_20newsgroups(subset='test',\n",
    "                remove=('headers','footers','quotes'),\n",
    "                categories=categories\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796803c4",
   "metadata": {},
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "- alt.atheism ë¬´ì‹ ë¡ , ì¢…êµ ë¹„íŒ í† ë¡  ì¢…êµì˜ ì¡´ì¬, ì‹ ì˜ ì¡´ì¬ ìœ ë¬´, ì¢…êµì  ì£¼ì¥ ë°˜ë°•, ì² í•™ì  ë…¼ìŸ ë“±\n",
    "- talk.religion.misc ì¼ë°˜ ì¢…êµ í† ë¡  (ê¸°íƒ€ ì¡ë‹´ í¬í•¨) ê¸°ë…êµ, ë¶ˆêµ, ì´ìŠ¬ëŒ ë“± ë‹¤ì–‘í•œ ì¢…êµ ê´€ë ¨ ì´ì•¼ê¸°, ê°œì¸ ê²½í—˜, ì‹ ë… ê³µìœ  ë“±\n",
    "- comp.graphics ì»´í“¨í„° ê·¸ë˜í”½ìŠ¤, ì´ë¯¸ì§€ ì²˜ë¦¬ 3D ë Œë”ë§, ì´ë¯¸ì§€ íŒŒì¼ í¬ë§·, ê·¸ë˜í”½ ì†Œí”„íŠ¸ì›¨ì–´ ì‚¬ìš©ë²•, OpenGL ë“± ê¸°ìˆ  ê´€ë ¨ í† ë¡ \n",
    "- sci.space ìš°ì£¼ ê³¼í•™, ì²œë¬¸í•™ ë¡œì¼“, NASA, í–‰ì„± íƒì‚¬, ì™¸ê³„ ìƒëª… ê°€ëŠ¥ì„±, ìš°ì£¼ ë¬¼ë¦¬í•™ ê´€ë ¨ í† ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b8611a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_files\n",
    "train_path = r'C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\LLM\\\\ch05\\\\20newsbydate\\\\20news-bydate-train'\n",
    "test_path = r'C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\LLM\\\\ch05\\\\20newsbydate\\\\20news-bydate-test'\n",
    "newsgroups_train = load_files(train_path,encoding='latin1')\n",
    "newsgroups_test = load_files(test_path,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f873e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af03a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¹´í…Œê³ ë¦¬ ì œê±°\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [ target_names.index(c) for c in categories  ]\n",
    "    #í•„í„°ë§\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text,label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # ë¼ë²¨ ì¬ ì •ë ¬\n",
    "            data_filtered.append(text) ; target_filtered.append( new_label  )\n",
    "    return data_filtered,target_filtered,categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f2403c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target, target_names = filter_categories(newsgroups_train,categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_train,categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02aa64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•´ë” í‘¸í„° ì¸ìš©ë¬¸ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f187acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # í—¤ë” ì œê±°\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # í’‹í„° ì œê±°\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # ì¸ìš©ë¬¸ ì œê±°\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab47bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [ clean_text(t) for t in train_data]\n",
    "test_data = [ clean_text(t) for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d264e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 2034, 2034, 2034)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_target), len(test_data), len(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00632272",
   "metadata": {},
   "source": [
    "# ë©€í‹°ë…¸ë©€ ë‚˜ì´ë¸Œë² ì´ì¦ˆ (Multinomial Naive Bayes)\n",
    "\n",
    "- ë¬¸ì„œì— í¬í•¨ëœ ë‹¨ì–´ë“¤ì˜ ì¶œí˜„ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì„œ ê·¸ ë¬¸ì„œê°€ ì–´ë–¤ ì£¼ì œì— ì†í• ì§€ í™•ë¥ ì ìœ¼ë¡œ ê³„ì‚°  \n",
    "- **ìŠ¤íŒ¸ í•„í„°ë§**, **ë‰´ìŠ¤ ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**, **ê°ì„± ë¶„ì„** ë“±ì— ì‚¬ìš©  \n",
    "- **ë² ì´ì¦ˆ ì •ë¦¬ í™•ë¥  ì´ë¡ ** ê¸°ë°˜ â†’ ì¡°ê±´ë¶€ í™•ë¥   \n",
    "- **ë‹¨ì–´ê°€ ë‚˜ì™”ì„ ë•Œ ì´ ë¬¸ì„œê°€ ìŠ¤íŒ¸ì¼ í™•ë¥ **ì„ ê³„ì‚°  \n",
    "\n",
    "\\[\n",
    "P(\\text{ìŠ¤íŒ¸}|\\text{ë‹¨ì–´ë“¤}) = \\frac{P(\\text{ë‹¨ì–´ë“¤}|\\text{ìŠ¤íŒ¸}) \\times P(\\text{ìŠ¤íŒ¸})}{P(\\text{ë‹¨ì–´ë“¤})}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ± ë‚˜ì´ë¸Œ (Naive): ìˆœì§„í•œ ê°€ì •\n",
    "\n",
    "- **ê°€ì •**: ë¬¸ì„œ ì•ˆì˜ ëª¨ë“  ë‹¨ì–´ëŠ” ì„œë¡œ ë…ë¦½ì ì´ë‹¤.  \n",
    "- **í˜„ì‹¤**: ìŠ¤íŒ¸ì— ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì€ ì„œë¡œ ë…ë¦½ì ì´ì§€ ì•Šë‹¤.  \n",
    "- **ì‹¤ì œ**: ì´ëŸ¬í•œ ê°€ì •ì€ ê³„ì‚°ëŸ‰ì„ ë¹ ë¥´ê²Œ í•˜ê³  ë‹¨ìˆœí•˜ì§€ë§Œ, ì–´ëŠ ì •ë„ ì •í™•ë„ë¥¼ ë³´ì¥í•œë‹¤.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ ë©€í‹°ë…¸ë©€: ë‹¤í•­ ë¶„í¬ ê¸°ë°˜\n",
    "\n",
    "- **ì˜ë¯¸**: ë‹¨ì–´ì˜ ì¶œí˜„ **íšŸìˆ˜**ë¥¼ ì¤‘ìš”í•˜ê²Œ ë³¸ë‹¤.  \n",
    "- íšŸìˆ˜ë¥¼ ì„¸ëŠ” ë©€í‹°ë…¸ë¯¸ì–¼ ë°©ì‹ì´ NLPì— ì˜ ë§ëŠ”ë‹¤.  \n",
    "- ëª¨ë¸ì€ ë‹¨ì–´ì˜ **ë¹ˆë„ìˆ˜ í†µê³„**ë¥¼ ì‚¬ìš©í•œë‹¤.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š ìŠ¤íŒ¸ ë©”ì¼ í†µê³„ (spam)\n",
    "\n",
    "| ë‹¨ì–´ | ë¹ˆë„ |\n",
    "|------|------|\n",
    "| free | 150  |\n",
    "| money | 100 |\n",
    "| viagra | 50 |\n",
    "| report | 5  |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š ì •ìƒ ë©”ì¼ í†µê³„ (ham)\n",
    "\n",
    "| ë‹¨ì–´ | ë¹ˆë„ |\n",
    "|------|------|\n",
    "| report | 80 |\n",
    "| meeting | 60 |\n",
    "| free | 10  |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ í™•ë¥  ê³„ì‚°\n",
    "\n",
    "ì´ëŸ¬í•œ í†µê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ, íŠ¹ì • ì¹´í…Œê³ ë¦¬(ì˜ˆ: ìŠ¤íŒ¸)ì—ì„œ ë‹¨ì–´ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•œë‹¤.\n",
    "\n",
    "\\[\n",
    "P(\\text{â€˜freeâ€™}|\\text{ìŠ¤íŒ¸})\n",
    "\\]\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ ë¬¸ì„œê°€ `\"Free money meeting\"`ì¼ ë•Œ:\n",
    "\n",
    "- ìŠ¤íŒ¸ì¼ í™•ë¥   \n",
    "  \\[\n",
    "  P(\\text{ìŠ¤íŒ¸}) \\times P(\\text{free}|\\text{ìŠ¤íŒ¸}) \\times P(\\text{money}|\\text{ìŠ¤íŒ¸}) \\times P(\\text{meeting}|\\text{ìŠ¤íŒ¸})\n",
    "  \\]\n",
    "\n",
    "- ì •ìƒì¼ í™•ë¥   \n",
    "  \\[\n",
    "  P(\\text{ì •ìƒ}) \\times P(\\text{free}|\\text{ì •ìƒ}) \\times P(\\text{money}|\\text{ì •ìƒ}) \\times P(\\text{meeting}|\\text{ì •ìƒ})\n",
    "  \\]\n",
    "\n",
    "ì´ ë‘ ê°’ì„ ë¹„êµí•˜ì—¬ ë” í° í™•ë¥ ì„ ê°€ì§„ ìª½ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363dae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sally\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk tokenizer stemer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd4b6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b22240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2034, 2000), (2034, 2000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_df : ë‹¨ì–´ì˜ ë¹ˆë„ê°€ ìµœì†Œ 5ê°œì˜ ë¬¸ì„œì— ë“±ì¥  - ë…¸ì´ì¦ˆ ê°ì†Œ\n",
    "# max_df : 50% ë„ˆë¬´í”í•œ ë‹¨ì–´ëŠ” ì œê±°\n",
    "cv = CountVectorizer(max_features=2000,min_df=5, max_df=0.5)\n",
    "x_train_cv = cv.fit_transform(train_data)\n",
    "x_test_cv = cv.transform(test_data)\n",
    "x_train_cv.shape,  x_test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3767f51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '01', ..., 'zip', 'zoo', 'zoology'],\n",
       "      shape=(2000,), dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c22bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_cv[0].toarray()[0])[x_train_cv[0].toarray()[0]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d106b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090462143559489 0.9090462143559489\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.90      0.90      0.90       480\n",
      "talk.religion.misc       0.87      0.83      0.85       377\n",
      "     comp.graphics       0.92      0.96      0.94       584\n",
      "         sci.space       0.93      0.92      0.93       593\n",
      "\n",
      "          accuracy                           0.91      2034\n",
      "         macro avg       0.90      0.90      0.90      2034\n",
      "      weighted avg       0.91      0.91      0.91      2034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bow ê¸°ë°˜\n",
    "# í…ìŠ¤íŠ¸ ë¶„ë¥˜ì˜ ê°•ë ¥í•œ baseline í¬ì†Œë°ì´í„°ì— ê°•í•¨\n",
    "# ëª¨ë¸ì„ íƒ\n",
    "nb = MultinomialNB()\n",
    "# í•™ìŠµìš© ë°ì´í„° ë²¡í„° ë°ì´í„°\n",
    "nb.fit(x_train_cv, train_target)\n",
    "\n",
    "print(nb.score(x_train_cv, train_target), nb.score(x_test_cv,test_target))\n",
    "\n",
    "# ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_nb = nb.predict(x_test_cv)\n",
    "print(classification_report(test_target,y_pred_nb, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533d63bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9287118977384464 0.9287118977384464\n"
     ]
    }
   ],
   "source": [
    "# TF_IDF + MNB + LogisticRegression\n",
    "# TF-IDF ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë‹¨ì–´ ê°•ì¡°, ì„ í˜•ëª¨ë¸ê³¼ ìì£¼ì‚¬ìš©, BOW ëŒ€ë¹„ í”í•œ ë‹¨ì–´ ì˜í–¥ ê°ì†Œ\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df = 0.5)\n",
    "x_train_tfid = tfidf.fit_transform(train_data)\n",
    "x_test_tfid = tfidf.transform(test_data)\n",
    "\n",
    "# NB + tf-idf\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(x_train_tfid, train_target)\n",
    "print(nb_tfidf.score(x_train_tfid, train_target), nb_tfidf.score(x_test_tfid, test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f1141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704978488014752 0.8894348894348895\n"
     ]
    }
   ],
   "source": [
    "# logistic \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test \\\n",
    "    = train_test_split(x_train_tfid,train_target,test_size=0.2\n",
    "                       ,stratify=train_target,random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(lr.score(x_train,y_train),  lr.score(x_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6664a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9317762753534112 0.8722358722358723\n"
     ]
    }
   ],
   "source": [
    "# ê³¼ì í•© í•´ê²°ì„ ìœ„í•œ ê·œì œ\n",
    "rc = RidgeClassifier(alpha=10)\n",
    "rc.fit(x_train, y_train)\n",
    "print(rc.score(x_train,y_train),  rc.score(x_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78503313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8893669330055316 0.8329238329238329\n"
     ]
    }
   ],
   "source": [
    "# L1 ê·œì œ   L1 Logistic(Lassoì™€ ìœ ì‚¬)\n",
    "# ì¼ë¶€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ íŠ¹ì„± ì„ íƒì„ ìˆ˜í–‰.. ì¤‘ìš”í”¼ì²˜ select íš¨ê³¼\n",
    "l1_lr = LogisticRegression(penalty='l1',max_iter=1000,solver='saga')\n",
    "l1_lr.fit(x_train,y_train)\n",
    "print(l1_lr.score(x_train,y_train),  l1_lr.score(x_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61518cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¸ë¦¬ëª¨ë¸ + tfidf\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "fores = RandomForestClassifier(max_depth=3)\n",
    "dg = GradientBoostingClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bbf8b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9477566072526121 0.828009828009828\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=2)\n",
    "gb.fit(x_train,y_train)\n",
    "print(gb.score(x_train,y_train),  gb.score(x_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57279377",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train_data,train_target\n",
    "         ,stratify=train_target,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ \n",
    "# RegexpTokenizer + stopwords + PorterStemmer\n",
    "english_stops = set(stopwords.words('english'))\n",
    "regtok = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "def custom_tokenizer(text):\n",
    "    toks = regtok.tokenize(text.lower())\n",
    "    toks = [t for t in toks if t not in english_stops]\n",
    "    toks = [PorterStemmer().stem(t) for t in toks]\n",
    "    return toks\n",
    "tfidf_custom = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000,min_df=5, max_df=0.5)\n",
    "x_train_tfidf_c = tfidf_custom.fit_transform(x_train)\n",
    "x_test_tfidf_c = tfidf_custom.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_tfidf_c,y_train)\n",
    "print( lr_c.score(x_train_tfidf_c, y_train) , lr_c.score(x_test_tfidf_c, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd18345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram ì‹¤í—˜ 1,2  1,3\n",
    "# ì„±ëŠ¥í–¥ìƒ ê¸°ëŒ€  ì—°ì†ëœ ë‹¨ì–´íŒ¨í„´ í¬ì°©\n",
    "tfidf_12 = TfidfVectorizer(token_pattern = r\"[\\w']{3,}\"\n",
    "                           ,stop_words= stopwords.words('english')\n",
    "                           ,ngram_range=(1,2)\n",
    "                           , min_df=2, max_df=0.5\n",
    "                        #    ,max_features=2000\n",
    "                           )\n",
    "x_train_12 = tfidf_12.fit_transform(x_train)\n",
    "x_test_12 = tfidf_12.transform(x_test)\n",
    "\n",
    "lr_c = LogisticRegression(max_iter=1000)\n",
    "lr_c.fit(x_train_12,y_train)\n",
    "print( lr_c.score(x_train_12, y_train) , lr_c.score(x_test_12, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ì²˜ë¦¬  konlpy\n",
    "# í’ˆì‚¬ê¸°ë°˜ íƒœê¹… tokenizer   Noun Verb Adjetive\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb782e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test =  train_test_split(df.review\n",
    "    ,df.title,stratify=df.title, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa69dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple version\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=okt.nouns, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c083cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf.shape  , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_train_tfidf,y_train), clf.score(x_test_tfidf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ceb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_tokenizer(text):\n",
    "    target = ['Noun','Verb','Adjective']\n",
    "    return [w for w,tag in okt.pos(text,norm= True, stem=True) if tag in target]\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(x_train_tfidf,y_train)\n",
    "clf.score(x_train_tfidf,y_train), clf.score(x_test_tfidf,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
