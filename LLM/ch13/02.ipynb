{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894c86bc",
   "metadata": {},
   "source": [
    "학습용 영어-프랑스어 병렬 문장 데이터 준비\n",
    "개념: \n",
    "   - 입력(영어)과 출력(프랑스어) 쌍으로 구성\n",
    "   - 디코더 입력에는 시작 토큰(\\t), 타겟에는 종료 토큰(\\n) 추가\n",
    " 설명:\n",
    "   - input_texts: 인코더에 입력될 영어 문장\n",
    "   - target_texts: 디코더가 생성해야 할 프랑스어 문장 (전처리 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c23b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력:Hello                --> 타겟 : \tBonjour\n",
      "\n",
      "입력:How are you          --> 타겟 : \tComment allez-vous\n",
      "\n",
      "입력:Good morning         --> 타겟 : \tBonjour matin\n",
      "\n",
      "입력:Thank you            --> 타겟 : \tMerci\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data_pairs = [\n",
    "    (\"Hello\", \"Bonjour\"),\n",
    "    (\"How are you\", \"Comment allez-vous\"),\n",
    "    (\"Good morning\", \"Bonjour matin\"),\n",
    "    (\"Thank you\", \"Merci\"),\n",
    "]\n",
    "\n",
    "# 입력과 타켓을 분리\n",
    "input_texts = []    \n",
    "target_texts = []\n",
    "\n",
    "# 입력에는 시작토큰, 타겟에는 종료토큰을 부여\n",
    "for eng, fra in data_pairs:\n",
    "    input_texts.append(eng)\n",
    "    # 디코더 입력 '\\t'(시작), 디코더 출력 '\\n'(종료)\n",
    "    target_texts.append(f'\\t{fra}\\n')\n",
    "for i in range(len(input_texts)):\n",
    "    print(f\"입력:{input_texts[i]:20s} --> 타겟 : {target_texts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f84981",
   "metadata": {},
   "source": [
    "문자 단위 사전(vocabulary) 생성 및 정수 인덱스 변환\n",
    "- 개념:\n",
    "    - 각 문자를 고유한 정수로 매핑\n",
    "    - 입력과 타겟의 사전은 별도 관리\n",
    "    - 원-핫 인코딩으로 신경망 입력 형태 생성\n",
    "- 설명:\n",
    "    - input_characters: 영어 문장에 등장하는 모든 고유 문자\n",
    "    - target_characters: 프랑스어 문장 + 특수 토큰(\\t, \\n)\n",
    "    - encoder_input_data: 3D 배열 (샘플, 시퀀스 길이, 문자 사전 크기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66ed8d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 입력 문자수 : 19\n",
      "고유 타겟 문자수 : 22\n",
      "최대 입력 문장길이 : 12\n",
      "최대 타겟 문장길이 : 20\n",
      "# 샘플, 시퀀스 길이, 문자 사전 크기\n",
      "encoder_input_data : (4, 12, 19)\n",
      "decoder_input_data : (4, 20, 22)\n",
      "decoder_target_data : (4, 20, 22)\n"
     ]
    }
   ],
   "source": [
    "# 입력과 타겟의 고유한 문자 수집\n",
    "# input_characters = set()\n",
    "# target_characters = set()\n",
    "\n",
    "# for text in input_texts:\n",
    "#     for char in text:\n",
    "#         input_characters.add(char)\n",
    "\n",
    "input_characters = {char for text in input_texts for char in text}\n",
    "target_characters = {char for target_text in target_texts  for char in target_text}\n",
    "\n",
    "# 정렬해서 일관성 확보\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters= sorted(list(target_characters))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# 가장 긴 문장 길이 계산\n",
    "max_encoder_seq_length = max(len(txt) for txt in input_texts  )\n",
    "max_decoder_seq_length = max(len(txt) for txt in target_texts  )\n",
    "# 문자-> 인덱스 매핑\n",
    "input_token_index = { char:i for i,char in enumerate(input_characters)}\n",
    "target_token_index = { char:i for i,char in enumerate(target_characters)}\n",
    "\n",
    "# 인덱스 -> 문자 역매핑(추론시 사용)\n",
    "reverse_input_token_index =  { idx:char for char,idx in input_token_index.items()}\n",
    "reverse_target_token_index = { idx:char for char,idx in target_token_index.items()}\n",
    "#  encoder_input_data: 3D 배열 (샘플, 시퀀스 길이, 문자 사전 크기)\n",
    "encoder_input_data = np.zeros( (len(input_texts),max_encoder_seq_length,num_encoder_tokens ),\n",
    "                              dtype='float32' )\n",
    "decoder_input_data = np.zeros( (len(input_texts),max_decoder_seq_length,num_decoder_tokens ),\n",
    "                              dtype='float32' )\n",
    "decoder_target_data = np.zeros( (len(input_texts),max_decoder_seq_length,num_decoder_tokens ),\n",
    "                              dtype='float32' )\n",
    "\n",
    "# 문자별 원핫 인코딩\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0    \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_input_data: 전체 타겟 시퀀스 (시작 토큰 포함)\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0        \n",
    "        # decoder_target_data: 한 타임스텝 앞선 정답 (Teacher Forcing용)\n",
    "        # 디코더 입력 \\t안녕\n",
    "        # 디코더 출력 안녕\\n\n",
    "        # 한 스텝 시프트 -  Teacher Forcing\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "# hi  입력\n",
    "# \\t hello  디코더 입력\n",
    "# hello \\n  디코더 출력 - Teacher Forcing  한스텝 앞으로 이동\n",
    "print(f'고유 입력 문자수 : {num_encoder_tokens}')\n",
    "print(f'고유 타겟 문자수 : {num_decoder_tokens}')\n",
    "print(f'최대 입력 문장길이 : {max_encoder_seq_length}')\n",
    "print(f'최대 타겟 문장길이 : {max_decoder_seq_length}')\n",
    "print('# 샘플, 시퀀스 길이, 문자 사전 크기')\n",
    "print(f'encoder_input_data : {encoder_input_data.shape}')\n",
    "print(f'decoder_input_data : {decoder_input_data.shape}')\n",
    "print(f'decoder_target_data : {decoder_target_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f940ec4",
   "metadata": {},
   "source": [
    "LSTM 기반 Seq2Seq 인코더-디코더 학습 모델 구축\n",
    "- 개념:\n",
    "    - Encoder: 입력 시퀀스를 처리하고 최종 상태(h, c) 출력\n",
    "    - Decoder: Encoder 상태를 초기값으로 받아 타겟 시퀀스 생성\n",
    "    - return_state=True: LSTM 내부 상태(h, c) 반환\n",
    "    - return_sequences=True: 모든 타임스텝 출력\n",
    "- 설명:\n",
    "    - encoder_states: [h, c] (hidden state, cell state)\n",
    "    - decoder_lstm: 초기 상태로 encoder_states 전달\n",
    "    - decoder_dense: Softmax로 각 타임스텝의 문자 확률 분포 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256  # LSTM 은닉 차원 (내부 표현 크기)\n",
    "\n",
    "# ==================== Encoder ====================\n",
    "encoder_inputs = input(shape=(None, num_encoder_tokens), name='encoder_input')\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# encoder_outputs는 사용하지 않고, 내부 상태(state_h, state_c)만 디코더로 전달\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# ==================== Decoder ====================\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,s return_state=True, name='decoder_lstm')\n",
    "\n",
    "# 디코더 초기 상태로 인코더 최종 상태 사용 (컨텍스트 전달)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# 각 타임스텝에서 문자 확률 분포 생성\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# ==================== 학습 모델 ====================\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_training')\n",
    "\n",
    "print(\"\\n 모델 구조:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f16aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
