{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5e7849",
   "metadata": {},
   "source": [
    "2025-11-13 \n",
    "- Seq2Seq(Sequence-to-Sequence) 문제 정의와 해결 전략 이해\n",
    "- 기본 순환신경망(RNN) 한계와 LSTM 개선 아이디어 파악\n",
    "- Encoder-Decoder 구조의 정보 흐름(컨텍스트 벡터/숨겨진 상태) 이해\n",
    "- Teacher Forcing 기법의 학습 안정화 역할 및 Trade-off 이해\n",
    "- Softmax와 CrossEntropy 손실의 수식 및 직관적 해석\n",
    "- Inference 단계(그리디 vs 빔 서치)의 실전 활용 전략\n",
    "- 전체 파이프라인을 하나의 흐름으로 연결하여 종합 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563560e",
   "metadata": {},
   "source": [
    "seq2seq\n",
    "- 영문장을 한국어로 번역, 질의 --> 답변생성, 문장요약 등.\n",
    "- why?\n",
    "    - 입력과 출력의 길이 다 다를 수 있는 문제(번역, 요약, 챗봇)\n",
    "    - 고정 크기의 피처 벡터는 순서정보와 문맥관계를 충분히 반영 못함\n",
    "    - 단순 분류모델은 시퀀스 간의 종속적 생성(토큰별 점진적 예측)에 부적합\n",
    "\n",
    "동작원리\n",
    "1. Encoder : 입력 토큰들을 순차적으로 처리 -> 마지막/전체 Hidden State 집약\n",
    "2. Context(요약표현) : 인코더의 정보를 압축\n",
    "3. Decoder : 압축한 Context + 이전에 생성한 토큰을 이용해서 다음 토큰을 반복생성한다.\n",
    "4. 종료 : 특별한 종료 조건 EoS(end of sequence) 토큰이 나올 때까지\n",
    "\n",
    "실제 사용 예시\n",
    "- 번역\n",
    "- 대화시스템\n",
    "- 문서요약\n",
    "- 코드자동생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336815bf",
   "metadata": {},
   "source": [
    "## 2. 인코더–디코더(Encoder–Decoder) 구조\n",
    "\n",
    "### (1) 개념 정의\n",
    "- **Encoder**: 입력 시퀀스를 순환신경망(RNN/LSTM/GRU)으로 처리해 **내부 상태(hidden state)** 또는 **컨텍스트 벡터(context vector)** 로 압축하는 모듈  \n",
    "- **Decoder**: Encoder가 전달한 컨텍스트를 기반으로 **출력 시퀀스를 생성**하는 모듈\n",
    "\n",
    "즉,  \n",
    "**Encoder = 이해(Encoding)**  \n",
    "**Decoder = 생성(Decoding)**  \n",
    "을 담당하며, 입력과 출력의 길이가 달라도 처리할 수 있는 구조를 제공한다.\n",
    "\n",
    "---\n",
    "\n",
    "### (2) 왜 필요한가?\n",
    "- **기능 분리(Understanding vs Generating)**  \n",
    "  입력 이해와 출력 생성을 독립적으로 설계할 수 있어 확장성이 높다.\n",
    "- **도메인 확장 용이**  \n",
    "  텍스트→텍스트, 텍스트→태그, 텍스트→명령 등 다양한 작업에 적용 가능.\n",
    "- **추가 모듈 삽입 용이**  \n",
    "  Attention·Copy Mechanism 등 고급 구조를 쉽게 결합할 수 있는 기반 역할.\n",
    "\n",
    "---\n",
    "\n",
    "### (3) 동작 원리\n",
    "\n",
    "#### 📌 Encoder 단계\n",
    "- 입력 시퀀스:  \n",
    "  \\[\n",
    "  x_1, x_2, \\ldots, x_T\n",
    "  \\]\n",
    "- 순환 구조(RNN/LSTM/GRU)  \n",
    "  \\[\n",
    "  h_t = f(x_t, h_{t-1})\n",
    "  \\]\n",
    "- 최종 상태  \n",
    "  - 기본 Seq2Seq: 마지막 hidden state \\(h_T\\) 를 Context Vector로 사용  \n",
    "  - (주의) Attention 모델은 전체 \\(h_1 \\ldots h_T\\) 를 활용\n",
    "\n",
    "#### 📌 Decoder 단계\n",
    "- 초기 Hidden State ← Encoder의 마지막 상태 \\(h_T\\)\n",
    "- 반복 생성:\n",
    "  \\[\n",
    "  y_t = \\text{Softmax}(W \\cdot h_t^{dec})\n",
    "  \\]\n",
    "- 다음 입력:\n",
    "  - **Teacher Forcing**: 정답(GT) 토큰 사용\n",
    "  - **Inference**: 이전 예측 사용\n",
    "\n",
    "---\n",
    "\n",
    "### (4) 실제 사용 예시\n",
    "\n",
    "| 작업 | Encoder 입력 | Decoder 출력 |\n",
    "|------|--------------|---------------|\n",
    "| 번역 | 영어 문장 토큰 | 한국어 문장 토큰 |\n",
    "| 질의응답 | 질문 토큰 | 답변 토큰 |\n",
    "| 요약 | 긴 문서 토큰 | 요약 문장 토큰 |\n",
    "| 형태소 분석 | 음절 시퀀스 | 품사 태그 시퀀스 |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
