{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d907a364",
   "metadata": {},
   "source": [
    "- TF-IDF : 텍스트 벡터화\n",
    "- PCA : 차원 축소\n",
    "- LSA : 잠재 의미 분석\n",
    "- t-SNE : 2D 시각화\n",
    "- 로지스틱회귀\n",
    "- 토큰화 & 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6d71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "# TF-IDF 행렬에 대해서 SVD를 적용\n",
    "# 단어와 문서 간의 숨겨진 의미 관계를 찾음\n",
    "# PCA 차이:\n",
    "    # PCA : 데이터 자체 분산 최대화\n",
    "    # LSA : 문서 - 단어형태의 의미 구조 파악\n",
    "# 은행\n",
    "    # 돈, 계좌 주변에 등장\n",
    "    # 나무 냄새 먹는다 주변에 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f2c2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE : 고차원 데이터를 2D/3D로 변환 - 시각화 전용(분석에는 부적합), 계산이 오래걸림\n",
    "# PCA vs. t-SNE\n",
    "# PCA : 속도가 빠름, 전역 구조 보존\n",
    "# t-SNE : 느림, 국소(지역) 군집 명확"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e02d3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋\n",
    "from sklearn.datasets import load_files\n",
    "train_path = r'C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\LLM\\\\ch05\\\\20newsbydate\\\\20news-bydate-train'\n",
    "test_path = r'C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\LLM\\\\ch05\\\\20newsbydate\\\\20news-bydate-test'\n",
    "newsgroups_train = load_files(train_path,encoding='latin1')\n",
    "newsgroups_test = load_files(test_path,encoding='latin1')\n",
    "categories =  ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 헤더 제거\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 풋터 제거\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 인용문 제거\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "\n",
    "    return text\n",
    "# 카테고리 제거\n",
    "def filter_categories(dataset, categories):\n",
    "    target_names = dataset.target_names\n",
    "    selected_idx = [ target_names.index(c) for c in categories  ]\n",
    "    #필터링\n",
    "    data_filtered, target_filtered = [], []\n",
    "    for text,label in zip(dataset.data, dataset.target):\n",
    "        if label in selected_idx:\n",
    "            new_label = selected_idx.index(label)  # 라벨 재 정렬\n",
    "            data_filtered.append(text) ; target_filtered.append( new_label  )\n",
    "    return data_filtered,target_filtered,categories\n",
    "train_data, train_target, target_names = filter_categories(newsgroups_train,categories)\n",
    "test_data, test_target, _ = filter_categories(newsgroups_test,categories)\n",
    "\n",
    "x_train = [ clean_text(t) for t in train_data]\n",
    "x_test = [ clean_text(t) for t in test_data]\n",
    "y_train = train_target\n",
    "y_test = test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f936016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 ( 소문자 + 토큰화(3글자이상) + 불용어제거(stopwords) + 어간추출(stemming))  --> 영어\n",
    "# 파이프라인 \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer  # 같은 의미의 다른형태 단어를 통일\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "regtok = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "english_stops = set(stopwords.words('english'))\n",
    "# 커스텀 토크나이져\n",
    "def tokenizer(text):\n",
    "    tokens = regtok.tokenize(text)\n",
    "    words = [word for word in tokens if word not in english_stops]\n",
    "    features = list(map(lambda x : PorterStemmer().stem(x), words))\n",
    "    return features\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer,max_features=2000,min_df=2,max_df=0.5)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "# 분류모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression(max_iter=200,random_state=42)\n",
    "lr_clf.fit(x_train_tfidf,y_train)\n",
    "\n",
    "print('기본분류------------')\n",
    "print(f'학습정확도 : {lr_clf.score(x_train_tfidf,y_train)}')\n",
    "print(f'테스트정확도 : {lr_clf.score(x_test_tfidf,y_test)}')\n",
    "\n",
    "print('주성분분석')\n",
    "# 데이터의 분산이 가장 큰 방향\n",
    "# 선형번환만 가능\n",
    "# 모든 데이터의 특성을 평등하게 고려\n",
    "# 용도 : 시각화, 속도 개선\n",
    "# 2000차원 - >100차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100,random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_tfidf.toarray())\n",
    "x_test_pca = pca.transform(x_test_tfidf.toarray())\n",
    "\n",
    "import numpy as np\n",
    "cumsum_var =  np.cumsum( pca.explained_variance_ratio_)\n",
    "print(f'원본 차원 : {x_train_tfidf.shape[1]}')\n",
    "print(f'축소후 차원 : {x_train_pca.shape[1]}')\n",
    "print(f'설명된 분산 : {pca.explained_variance_.sum()}')\n",
    "print(f'누적 분산 : {cumsum_var[:10]}')\n",
    "\n",
    "# pca 후 분류\n",
    "lr_clf_pca = LogisticRegression(max_iter=200,random_state=42)\n",
    "lr_clf_pca.fit(x_train_pca,y_train)\n",
    "\n",
    "print('주성분 분석 분류------------')\n",
    "print(f'학습정확도 : {lr_clf_pca.score(x_train_pca,y_train)}')\n",
    "print(f'테스트정확도 : {lr_clf_pca.score(x_test_pca,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa  잠재적 의미분석\n",
    "# tf-idf svd\n",
    "# PCA보다 의미론적 관계 잘 포착\n",
    "# 테스트 데이터에 효과적\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100,random_state=42)\n",
    "x_train_lsa = svd.fit_transform(x_train_tfidf)\n",
    "x_test_lsa = svd.transform(x_test_tfidf)\n",
    "\n",
    "print(f'원본 차원 : {x_train_tfidf.shape[1]}')\n",
    "print(f'축소후 차원 : {x_train_lsa.shape[1]}')\n",
    "print(f'설명된 분산 : {svd.explained_variance_.sum()}')\n",
    "\n",
    "# lsa 후 분류\n",
    "lr_clf_lsa = LogisticRegression(max_iter=200,random_state=42)\n",
    "lr_clf_lsa.fit(x_train_lsa,y_train)\n",
    "\n",
    "print('주성분 분석 분류------------')\n",
    "print(f'학습정확도 : {lr_clf_lsa.score(x_train_lsa,y_train)}')\n",
    "print(f'테스트정확도 : {lr_clf_lsa.score(x_test_lsa,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2201ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA 기반 유사도 분석\n",
    "# 코사인 유사도  0(완전히 다름) ~ 1(같음)\n",
    "# LSA 의미기반 유사도\n",
    "# 첫번재 문서의 유사 문서 찾기\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_result =  cosine_similarity([x_train_lsa[0]], x_train_lsa)\n",
    "sim_index = (-sim_result[0]).argsort()[:20]\n",
    "print(f'원문 카테고리 :{newsgroups_train.target_names[y_train[0]]}')\n",
    "print(f'상위 유사도 : {sorted(sim_result[0],reverse=True)[:5]}')\n",
    "similar_categorys =  [newsgroups_train.target_names[y_train[i]] for i in sim_index]\n",
    "print(f'유사 문서들의 카테고리 : {similar_categorys[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-sne  2D 시각화\n",
    "# 고차원 데이터를 2D/3D 변환\n",
    "# 비슷한 데이터는 가깝게 다른데이터는 멀리 배치\n",
    "# 계산이 오래 걸리지만 시각화 효과 가 뛰어남\n",
    "# 주의 : 분석용이 아니라 시각화 전용\n",
    "from sklearn.manifold import TSNE\n",
    "# perplexity 이웃개수 조절 \n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tnse_lsa = tsne.fit_transform(x_train_lsa)\n",
    "print(f'입력 차원 : {x_train_lsa.shape[1]}')\n",
    "print(f'출력 차원 : {tnse_lsa.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = y_train[:len(tnse_lsa)]\n",
    "plt.scatter(tnse_lsa[:,0],tnse_lsa[:,1],c = colors, cmap='viridis',alpha=0.6,s=50)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c05a0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF(원본데이터)\n",
    "    # 장점 : 해석이 쉽고 정확도가 높은\n",
    "    # 단점 : 메모리 많이 사용(차원이 높음-> 차원의 저주)\n",
    "    # 용도 : 정확도가 우선일 때\n",
    "# PCA(선형 차원 축소)\n",
    "    # 장점 : 계산이 빠르고 간단, 분산 최대 보존\n",
    "    # 단점 : 선형 변환만 가능\n",
    "    # 용도 : 속도와 정확도 균형\n",
    "# LSA(의미 기반 차원 축소)\n",
    "    # 장점 : 의미관계 잘 포착, 유사도 계산에 효과적\n",
    "    # 단점 : 계산비용 중간\n",
    "    # 용도 : 문서 유사도,추천 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ed89b",
   "metadata": {},
   "source": [
    "```\n",
    "텍스트 분류\n",
    "정확도 최우선           속도 중요(메모리)      시각화 필요\n",
    "TF-IDF                     PCA               t-SNE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e283f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 데이터\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f96344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터(문장)\n",
    "# 문장 ->벡터\n",
    "    # 문장 -> 토큰화\n",
    "        # 다양한 토크나이져\n",
    "            # 한글 Okt\n",
    "                # Okt().pos(문장)\n",
    "                # 불용어 제거 - 불용어 리스트를 만들어서 stopword 하듯이 제거\n",
    "    # 토큰 - > 벡터화\n",
    "        # BoW(CounterVectorizer)\n",
    "        # TF-IDF\n",
    "# 분류모델 선택\n",
    "# 학습\n",
    "# 평가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
