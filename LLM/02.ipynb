{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b02cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sally\\sknpyworkspace\\streamlit\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~onlpy (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pype1 (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~onlpy (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pype1 (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~onlpy (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~pype1 (c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f19349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 영화 리뷰 수: 2000\n",
      "카테고리: ['neg', 'pos']\n",
      "부정 리뷰: 1000개\n",
      "긍정 리뷰: 1000개\n",
      "\n",
      "첫 번째 리뷰 ID: neg/cv000_29416.txt\n",
      "원문 일부:\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "w\n",
      "\n",
      "문장 토큰화 (첫 2개):\n",
      "  1: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.']\n",
      "  2: ['they', 'get', 'into', 'an', 'accident', '.']\n",
      "\n",
      "단어 토큰화 (첫 20개): ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\sally\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sally\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1단계: 필요한 데이터 다운로드\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2단계: 데이터 탐색\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 데이터셋 크기 파악\n",
    "print(f\"전체 영화 리뷰 수: {len(movie_reviews.fileids())}\")\n",
    "print(f\"카테고리: {movie_reviews.categories()}\")  # ['neg', 'pos']\n",
    "print(f\"부정 리뷰: {len(movie_reviews.fileids(categories='neg'))}개\")\n",
    "print(f\"긍정 리뷰: {len(movie_reviews.fileids(categories='pos'))}개\")\n",
    "\n",
    "# 3단계: 첫 번째 리뷰 살펴보기\n",
    "first_review_id = movie_reviews.fileids()[0]\n",
    "first_review = movie_reviews.raw(first_review_id)\n",
    "print(f\"\\n첫 번째 리뷰 ID: {first_review_id}\")\n",
    "print(f\"원문 일부:\\n{first_review[:200]}\")\n",
    "\n",
    "# 4단계: 토큰화 결과 확인\n",
    "sentences = movie_reviews.sents(first_review_id)  # 문장 단위 토큰화\n",
    "words = movie_reviews.words(first_review_id)      # 단어 단위 토큰화\n",
    "\n",
    "print(f\"\\n문장 토큰화 (첫 2개):\")\n",
    "for i, sent in enumerate(sentences[:2]):\n",
    "    print(f\"  {i+1}: {sent}\")\n",
    "\n",
    "print(f\"\\n단어 토큰화 (첫 20개): {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6975acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer: 정규표현식으로 정확한 토큰화\n",
    "# stopwords : 문법적 기능을 제거하고 단어에 집중\n",
    "# 상위 N개 단어 선택 : 메모리효율성과 노이즈 제거의 균형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db15b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 2000\n",
      "첫 문서의 단어 수: 879\n",
      "첫 문서의 첫 50개 단어:\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n",
      "\n",
      "상위 10개 빈도 단어:\n",
      "  1. ',': 77717회\n",
      "  2. 'the': 76529회\n",
      "  3. '.': 65876회\n",
      "  4. 'a': 38106회\n",
      "  5. 'and': 35576회\n",
      "  6. 'of': 34123회\n",
      "  7. 'to': 31937회\n",
      "  8. ''': 30585회\n",
      "  9. 'is': 25195회\n",
      "  10. 'in': 21822회\n",
      "\n",
      "전체 서로 다른 단어 수: 43011\n",
      "\n",
      "처리 후 상위 10개 단어:\n",
      "  1. 'film': 8935회\n",
      "  2. 'one': 5791회\n",
      "  3. 'movie': 5538회\n",
      "  4. 'like': 3690회\n",
      "  5. 'even': 2564회\n",
      "  6. 'time': 2409회\n",
      "  7. 'good': 2407회\n",
      "  8. 'story': 2136회\n",
      "  9. 'would': 2084회\n",
      "  10. 'much': 2049회\n",
      "\n",
      "특성으로 선택된 단어 수: 1000\n",
      "특성 예시: ['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']\n"
     ]
    }
   ],
   "source": [
    "# BOW - 수동으로 벡터 생성\n",
    "# 1단계: 모든 문서를 단어 리스트로 변환\n",
    "documents = [list(movie_reviews.words(fileid)) \n",
    "             for fileid in movie_reviews.fileids()]\n",
    "\n",
    "print(f\"전체 문서 수: {len(documents)}\")\n",
    "print(f\"첫 문서의 단어 수: {len(documents[0])}\")\n",
    "print(f\"첫 문서의 첫 50개 단어:\\n{documents[0][:50]}\")\n",
    "\n",
    "# 2단계: 전체 단어 빈도 계산 (불용어 제외 전)\n",
    "word_count = {}\n",
    "for doc in documents:\n",
    "    for word in doc:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "# 상위 10개 빈도 단어 확인\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n상위 10개 빈도 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 3단계: 불용어 제거 후 처리\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 정규표현식으로 3글자 이상의 단어만 추출\n",
    "tokenizer = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "# 영어 불용어 로드\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# 모든 리뷰를 토큰화하고 불용어 제거\n",
    "processed_documents = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    raw_text = movie_reviews.raw(fileid)\n",
    "    tokens = [token for token in tokenizer.tokenize(raw_text) \n",
    "              if token not in english_stops]\n",
    "    processed_documents.append(tokens)\n",
    "\n",
    "# 처리 후 단어 빈도 재계산\n",
    "word_count_processed = {}\n",
    "for doc in processed_documents:\n",
    "    for word in doc:\n",
    "        word_count_processed[word] = word_count_processed.get(word, 0) + 1\n",
    "\n",
    "sorted_processed = sorted(word_count_processed.items(), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n전체 서로 다른 단어 수: {len(sorted_processed)}\")\n",
    "print(\"\\n처리 후 상위 10개 단어:\")\n",
    "for i, (word, count) in enumerate(sorted_processed[:10], 1):\n",
    "    print(f\"  {i}. '{word}': {count}회\")\n",
    "\n",
    "# 4단계: 특성 선택 (상위 1000개 단어)\n",
    "word_features = [word for word, count in sorted_processed[:1000]]\n",
    "print(f\"\\n특성으로 선택된 단어 수: {len(word_features)}\")\n",
    "print(f\"특성 예시: {word_features[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ffdb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 130 262 278 391 "
     ]
    }
   ],
   "source": [
    "# processed_documents[0]  # 문장을 토큰화(3개의 연속된 문장, 불용어제거)\n",
    "for doc in processed_documents[:5]:\n",
    "    print(len(doc), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c90583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', 'two', 'teen']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb4a2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 단어 리스트: ['one', 'two', 'teen', 'couples', 'solo']\n",
      "테스트 문서: ['two', 'two', 'couples']\n",
      "결과 벡터: [0, 2, 0, 1, 0]\n",
      "→ 'two'가 2번, 'couples'가 1번, 나머지는 0\n",
      "\n",
      "생성된 특성 벡터 수: 2000\n",
      "각 벡터의 차원: 1000\n",
      "\n",
      "첫 문서 벡터 (처음 20개):\n",
      "  'film': 5\n",
      "  'one': 3\n",
      "  'movie': 6\n",
      "  'like': 3\n",
      "  'even': 3\n",
      "  'time': 0\n",
      "  'good': 2\n",
      "  'story': 0\n",
      "  'would': 1\n",
      "  'much': 0\n",
      "  'also': 1\n",
      "  'get': 3\n",
      "  'character': 1\n",
      "  'two': 2\n",
      "  'well': 1\n",
      "  'first': 0\n",
      "  'characters': 1\n",
      "  'see': 2\n",
      "  'way': 3\n",
      "  'make': 5\n"
     ]
    }
   ],
   "source": [
    "# 각 문서의 고정된 길이의 벡터로 변환(모든 문서가 같은 차원 )\n",
    "# 기계학습 알고리즘의 입력 형식으로 변환\n",
    "def document_features(document, word_features):\n",
    "    \"\"\"\n",
    "    문서를 특성 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        document: 토큰화된 단어 리스트\n",
    "        word_features: 특성으로 사용할 단어 리스트\n",
    "    \n",
    "    Returns:\n",
    "        document의 각 특성에 대한 빈도 리스트\n",
    "    \"\"\"\n",
    "    # 문서 내 단어 빈도 계산\n",
    "    word_count = {}\n",
    "    for word in document:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    \n",
    "    # 특성 벡터 생성\n",
    "    features = []\n",
    "    for word in word_features:\n",
    "        # 특성 단어가 문서에 없으면 0\n",
    "        features.append(word_count.get(word, 0))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 테스트 실행\n",
    "test_features = ['one', 'two', 'teen', 'couples', 'solo']\n",
    "test_doc = ['two', 'two', 'couples']\n",
    "result = document_features(test_doc, test_features)\n",
    "\n",
    "print(\"테스트 단어 리스트:\", test_features)\n",
    "print(\"테스트 문서:\", test_doc)\n",
    "print(\"결과 벡터:\", result)\n",
    "print(\"→ 'two'가 2번, 'couples'가 1번, 나머지는 0\")\n",
    "\n",
    "# 모든 문서에 대해 특성 벡터 생성\n",
    "feature_sets = [document_features(doc, word_features) \n",
    "                 for doc in processed_documents]\n",
    "\n",
    "print(f\"\\n생성된 특성 벡터 수: {len(feature_sets)}\")\n",
    "print(f\"각 벡터의 차원: {len(feature_sets[0])}\")\n",
    "print(f\"\\n첫 문서 벡터 (처음 20개):\")\n",
    "for i, (word, count) in enumerate(zip(word_features[:20], feature_sets[0][:20])):\n",
    "    print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58667af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['film', 'one', 'movie'], [5, 3, 6], 1000, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:3],  feature_sets[0][:3],  len(word_features), len(feature_sets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3748e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성수 : ['film' 'one' 'movie' 'like' 'even']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 데이터 준비\n",
    "reivews = [  movie_reviews.raw(fileid) for fileid in movie_reviews.fileids() ]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(vocabulary=word_features)\n",
    "reviews_cv = cv.fit_transform(reivews)\n",
    "print(f'특성수 : {cv.get_feature_names_out()[:5]}')\n",
    "reviews_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5ceada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o/view?usp=sharing\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5569c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o/view?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e1fc66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review  rating        date   title\n",
       "0                        돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29  인피니티 워\n",
       "1  몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26  인피니티 워"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cad188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "okt = Okt()\n",
    "def custom_tokenizer(doc):\n",
    "    \"\"\"\n",
    "    형태소 분석 후 명사, 동사, 형용사만 추출\n",
    "    \"\"\"\n",
    "    pos_tags = okt.pos(doc)\n",
    "    tokens = [word for word, pos in pos_tags \n",
    "              if pos in ['Noun', 'Verb', 'Adjective']]\n",
    "    return tokens\n",
    "\n",
    "daum_cv = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "\n",
    "reviews = df.review\n",
    "daum_dtm  = daum_cv.fit_transform(reviews)\n",
    "original_review = reviews[0]  # 첫 번째 리뷰\n",
    "print(f\"원본 리뷰 (처음 200자):\\n{original_review[:200]}\\n\")\n",
    "\n",
    "# 문서의 뒤 절반을 query로 사용 (부분 검색 시나리오)\n",
    "midpoint = len(original_review) // 2\n",
    "query_text = original_review[midpoint:]  # 뒤 절반\n",
    "print(f\"쿼리 텍스트 (처음 150자):\\n{query_text[:150]}\\n\")\n",
    "\n",
    "# 2단계: 쿼리 문서를 벡터로 변환\n",
    "query_vector = daum_cv.transform([query_text])\n",
    "print(f\"쿼리 벡터 크기: {query_vector.shape}\")\n",
    "\n",
    "# 유사도 분석\n",
    "scores = cosine_similarity(query_vector,daum_dtm)\n",
    "most_simular_idx = np.argmax(scores)\n",
    "# scores[most_simular_idx], reviews[most_simular_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df28d312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12500, 11750, 12517,  3023,     0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0].argsort()[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0491c1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14725, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4017eb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'만 보는 내내 하품만'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f3021d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['보는 내내 눈시울이 ㅠㅠ', '여전한 군바리 국가지배...보는 내내 슬펐다.', '보는 내내 너무 괴로웠다',\n",
       "       '보는 내내 설레였다', '돈 들인건 티가 나지만 보는 내내 하품만'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(reviews)[scores[0].argsort()[::-1][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad9d95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선 TF-IDF\n",
    "# 단어의 상대적 중요도를 반영한 벡터화 기법\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "# TF란 특정 단어가 문서에서 얼마나 자주 나타나는지 비율\n",
    "# 해당 단어의 빈도 / 문서의 전체 단어 수 \n",
    "    # '좋다' 문서에서 10번 -> 해당 문서는 100단어 -> 10/100\n",
    "# IDF(Inverse Document Frequency) : 단어가 전체 문장에서 얼마나 희귀한지, 드문지\n",
    "    # IDF는 가중치를 낮추는 것\n",
    "# 전체 문서 / 해당 단어 포함 문서\n",
    "# log (전체 문서 / 해당 단어 포함 문서) 단어의 가중치를 낮추기 위해서 log 적용\n",
    "# 2000개 문서 중 100만 '좋다' log(2000/100) = 2.99\n",
    "# TF-IDF = TF x IDF --> 특정 문서에서 의미있는 단어에 높은 가중치 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cd9a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14725, 1000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_cv = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "tfid_dtm = tfidf_cv.fit_transform(reviews)\n",
    "tfid_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "506ce0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sally\\SKNPYWORKSPACE\\streamlit\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14725, 1000), (14725, 1000))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리 벡터화\n",
    "count_cv = CountVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "count_dtm = count_cv.fit_transform(reviews)\n",
    "tfidf_cv = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    tokenizer=custom_tokenizer\n",
    ")\n",
    "tfid_dtm = tfidf_cv.fit_transform(reviews)\n",
    "count_dtm.shape, tfid_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fd72e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당되는 문서에 대해서 쿼리 벡터화\n",
    "query_count = count_cv.transform([query_text])\n",
    "query_tfidf = tfidf_cv.transform([query_text])\n",
    "# 코사인 유사도 계산\n",
    "count_sim = cosine_similarity(query_count,count_dtm)[0]\n",
    "tfid_sim = cosine_similarity(query_count,tfid_dtm)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9f1fa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'만 보는 내내 하품만'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7689cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "a.argsort()     # 오름차순으로 인덱스\n",
    "(-a).argsort()  # 값의 마이너스는 내림차순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bac2888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_count_index = (-count_sim).argsort()[:5]\n",
    "top5_tfidf_index = (-tfid_sim).argsort()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "051de5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3023                    보는 내내 설레였다\n",
       "11750    여전한 군바리 국가지배...보는 내내 슬펐다.\n",
       "12500                보는 내내 눈시울이 ㅠㅠ\n",
       "12517                보는 내내 너무 괴로웠다\n",
       "0           돈 들인건 티가 나지만 보는 내내 하품만\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[top5_count_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bc07a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11750    여전한 군바리 국가지배...보는 내내 슬펐다.\n",
       "12517                보는 내내 너무 괴로웠다\n",
       "12500                보는 내내 눈시울이 ㅠㅠ\n",
       "12172         영화 보는 내내  분노했고 미안했다.\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[top5_tfidf_index][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_review = '숙면을 하기 좋은 영화, 강추'\n",
    "my_review count 방식이나 또는 tf-idf 방식으로 벡터화 한 후 .. 전체 리뷰ㅣ를 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
