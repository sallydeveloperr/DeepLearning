{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf5c3af",
   "metadata": {},
   "source": [
    "##### 감성분석 -> 머신러닝\n",
    "- 데이터셋 : 전처리\n",
    "- BoW 모델\n",
    "    - 단어를 특성 벡터로 변환\n",
    "    - tf-idf 단어 적합성 평가\n",
    "    - 텍스트 데이터 정제\n",
    "    - 문서를 토큰으로 나누기\n",
    "- LogisticRegession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92896a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋\n",
    "# http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d6c91f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Story of a man who has unnatural feelings for ...       0\n",
       "1  Airport '77 starts as a brand new luxury 747 p...       0\n",
       "2  This film lacked something I couldn't put my f...       0\n",
       "3  Sorry everyone,,, I know this is supposed to b...       0\n",
       "4  When I was little my parents took me along to ...       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "file_list = glob('C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\data\\\\movie\\\\train\\\\neg\\\\*txt')\n",
    "pd_lists = []\n",
    "for file_path in file_list[:500]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'review' : f.read(),\n",
    "            'target' : 0\n",
    "        }\n",
    "        df = pd.DataFrame([data])\n",
    "        pd_lists.append(df)\n",
    "\n",
    "train_neg_df = pd.concat(pd_lists, ignore_index=True)\n",
    "train_neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d2f79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive 동일하게 train_pos_df\n",
    "# train_df = pd.concat([train_neg_df, train_pos_df])\n",
    "# movie_data.csv로 저장\n",
    "\n",
    "# 긍정 리뷰 불러오기\n",
    "file_list = glob('C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\data\\\\movie\\\\train\\\\pos\\\\*txt')\n",
    "pd_lists = []\n",
    "for file_path in file_list[:500]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = {\n",
    "            'review': f.read(),\n",
    "            'target': 1   # 긍정 레이블\n",
    "        }\n",
    "        df = pd.DataFrame([data])\n",
    "        pd_lists.append(df)\n",
    "\n",
    "train_pos_df = pd.concat(pd_lists, ignore_index=True)\n",
    "\n",
    "# 부정 + 긍정 합치기\n",
    "train_df = pd.concat([train_neg_df, train_pos_df], ignore_index=True)\n",
    "train_df.head()\n",
    "\n",
    "# CSV 저장\n",
    "train_df.to_csv('C:\\\\Users\\\\sally\\\\SKNPYWORKSPACE\\\\data\\\\movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a85601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW(Bag of Words) 모델\n",
    "# 문자를 숫자벡터\n",
    "# 단어의 등장횟수를 카운트\n",
    "\n",
    "# 전체 훈련데이터에서 모든 고유한 단어(토큰)로 어휘 사전\n",
    "# 각 문서(리뷰데이터)를 사전을 기준으로 벡터화  N번째단어가 문서에서 3번나오면 벡터의 N번째값이 3이 된다.\n",
    "# 문서1 : \"나는 영화가 좋다\"\n",
    "# 문서2 : \"나는 영화가 싫다\"\n",
    "# 사전 : {'나는':0, '영화가':1,'좋다':2,'싫다':3}\n",
    "# 벡터화는 사전의 크기만큼 모든 문장의 길이를 동일하게\n",
    "# 문서1벡터 : [0,1,2] - > [1,1,1,0]\n",
    "# 문서2벡터 : [0,1,3] - > [1,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    \"The sun is shining\",\n",
    "    \"The weather is sweet\"\n",
    "])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "557fb000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 4, 'sun': 2, 'is': 0, 'shining': 1, 'weather': 5, 'sweet': 3}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c12eac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 d에 등장한 단어 t의 횟수를 tf(t,d)\n",
    "# BoW를 보완하면서 좀더 정교환 텍스트 벡터화 방식  TF-IDF(Term Frequency -Inverse Document Frequency)\n",
    "# TF : 특정문서에서 자주 등장하는 단어\n",
    "# IDF : 전체문서에 드물게 등장하는 단어\n",
    "# 특정문서에서 자주 등장하지만 전체 문장에서 드물게 등장하는 단어에 높은 가중치를 부여 - 그 문장을 잘 대표하는 핵심 단어를 찾는다\n",
    "# TF(t,d)  단어 t가 문장 d에 나타난 횟수 / 문서d의 모든 단어수\n",
    "# IDF(t,D) : log( 총 문서수|D| / 단어 t를 포함한 문서의수 df(t) )  -- log 단어의 희귀성을 너무 과하게 반영하지 않도록 스케일링\n",
    "# 분모에 + 1(사이킷런의 경우) :  분모가 0되는 것을 방지\n",
    "# log( 1+ |D| / 1+df(f)  )\n",
    "\n",
    "#TF-IDF(t,d,D) = TF(t,d) x IDF(t,D)\n",
    "\n",
    "# \"나는\"\n",
    "# TF : 리뷰에 3번 나옴 (높음)\n",
    "# IDF : 전체 10,000개 리뷰중에 9000개 나옴(매우 낮음)\n",
    "# tf-idf  높음 x 매우낮음 = 낮음(중요도가 낮음)\n",
    "\n",
    "# \"명작\" TF : 리뷰에 2번 나옴 (높음)\n",
    "# IDF : 전체 10,000개 리뷰중에 50개 나옴(매우 높음)\n",
    "# tf-idf  높음 x 매우높음 = 높음(핵심단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9eb952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정재... html tag 와 같은 불필요한 string이 보임... 특수기호 기타등등.  < - ' \n",
    "import re\n",
    "def preprocessor(s):\n",
    "    # 1. 영문, 공백, ., , 만 남기기\n",
    "    clean = re.sub(r'[^A-Za-z\\s.,]+', '', s)\n",
    "    # 2. 연속된 마침표(...)를 마침표 하나로\n",
    "    clean = re.sub(r'\\.{2,}', '.', clean)\n",
    "    # 3. 연속된 공백 정리\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b304fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df.review.apply(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3f47c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 토큰으로 나누기\n",
    "# %pip install nltk\n",
    "# %conda install -c conda-forge nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62220b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'criticisms',\n",
       " 'on',\n",
       " 'this',\n",
       " 'thread',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'pick',\n",
       " 'a',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'this',\n",
       " 'film',\n",
       " 'with',\n",
       " 'The',\n",
       " 'Mortal',\n",
       " 'Storm',\n",
       " 'o']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# 어간추출 Stemming  단어의 접미사 -s -es -ing -ed 등등.. 를 강제로 제거해서 단어의 원형을 찾는과정\n",
    "tokenizer(df.review[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2c2009d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('runners like running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sally\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')    # 불용어 사전 다운로드\n",
    "stops =  stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403193e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # 전처리완료  정규식을 이용한.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.review\n",
    "y = df.target\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간추출(postStemmer) -> stopwor에 포함된 단어제거\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split() if word not in stops]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=tokenizer_porter,\n",
    "    ngram_range=(1,1) # (1,1) 유니그램(unigramm, 단일단어)만 사용\n",
    ")\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140fa8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pipeline.predict(\u001b[43mx_test\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본데이터로드..\n",
    "# 토크나이져 함수를 정의\n",
    "    # 텍스트 전처리\n",
    "    # 공백을 기준으로 단어단위로 분리\n",
    "    # 영어는 전부 소문자로 변환\n",
    "    # 어간 추출\n",
    "    # 불용어 제거\n",
    "# TFIDF를 정의\n",
    "    # 토크나이져 매개변수 = 토크나이져 함수\n",
    "    # ngram  (1,1)\n",
    "# 파이프라인으로 tfidf, 머신러닝\n",
    "# 파이프라이으로 학습\n",
    "# 파이프라인으로 평가( classification_report)\n",
    "# 과적합여부 확인\n",
    "\n",
    "# train 폴더에 있는데이터로 학습 - 적당한 크기로\n",
    "# test 폴더에 있는 문장으로 평가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
